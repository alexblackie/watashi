<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Alex Blackie's Blog</title>
<link>https://www.alexblackie.com/</link>
<description>A RSS feed for all my published articles.</description>
<language>en-GB</language>
<atom:link href="https://www.alexblackie.com/feed.xml" rel="self" type="application/rss+xml" />

			<item>
<title>dnf, where art thou?</title>
<link>https://www.alexblackie.com/articles/dnf-where-art-thou/</link>
<guid>https://www.alexblackie.com/articles/dnf-where-art-thou/</guid>
<pubDate>Wed, 10 Jul 2019 09:00:00 GMT</pubDate>
<author>alex@alexblackie.com (Alex Blackie)</author>
<description></description>
<content:encoded><![CDATA[ 		<p>
	For a while now I have done all my dayjob work within a VM over SSH and
	Samba. This has worked quite well, and meant I could keep my development
	environment a bit more slow-moving (generally, the latest Ubuntu LTS) while
	running a faster-moving desktop distribution on the hardware itself. It
	also means when I travel I can <code>rsync</code> the VM to my personal
	laptop and not have to lug two laptops with me everywhere just to keep that
	work/personal separation.
</p>

<p>
	<strong>So. I tried to install updates</strong>. About halfway through
	installing updates, something I do a couple times a week usually, the VM
	froze. The SSH connection was responsive but I could tell all disk I/O was
	totally dead, and basically nothing worked -- not even closing a shell
	session. I opened the VM's console display and sure enough there was a
	screen full of watchdog warnings saying the Kernel had hung. I gave it a
	couple minutes but it seemed clear that something had gone horribly wrong.
</p>

<p>
	As it turns out, the virtual disk image I had copied from another computer
	was in VirtualBox format, and something in the driver in QEMU/KVM would
	cause it to completely stall periodically. Of course, this failure occurred
	at the worst time -- in the middle of installing updates -- and so this is
	my tale of trying to repair the VM once I rebooted it.
</p>

<hr>

<div class="code">
	<div class="code-title">~ % sudo apt update</div>
<pre><code>Hit:1 http://archive.ubuntu.com/ubuntu bionic InRelease
Hit:2 http://archive.ubuntu.com/ubuntu bionic-security InRelease
Hit:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease
Reading package lists... Done
Building dependency tree
Reading state information... Done
All packages are up to date.</code></pre></div>

<p>
	As always, make sure to glare disapprovingly at the <code>http://</code>
	plastered across your screen, knowing that even if you added an
	<code>s</code> it would not work because no one seems to provide secure
	Debian package mirrors, and unless you installed some extension package
	(over plain HTTP of course) <code>apt</code> doesn't even support secure
	connections.
</p>

<p>
	<code>All packages are up to date.</code> Um, well, that's entirely false.
	As soon as I saw this I knew I was in for a wild ride, and started thinking
	through how much work it would be to just rebuild the VM from scratch.
</p>

<p>
	I lost my shell history for this part but I had to eventually find out to
	run some <code>dpkg</code> command to "resume" the in-flight upgrade:
</p>

<pre><code>dpkg --configure --some-other-flags-i-think</code></pre>

<p>
	That did a bunch of stuff, seemingly finished the upgrade successfully.
</p>

<p>
	&ldquo;Well, let's just try again!&rdquo;
</p>

<div class="code">
	<div class="code-title">~ % sudo apt dist-upgrade</div>
<pre><code>Reading package lists... Done
Building dependency tree
Reading state information... Done
E: The package libgl1-mesa-dri needs to be reinstalled, but I can't find an archive for it.</code></pre></div>

<p>
	At least we're getting an error again.
</p>

<p>
	I first tried <code>apt reinstall</code> which I always forget does not
	exist. I moved on to just trying to remove and install it myself&hellip;
</p>

<div class="code">
	<div class="code-title">~ % sudo apt remove libgl1-mesa-dri</div>
<pre><code>Reading package lists... Done
Building dependency tree
Reading state information... Done
E: The package libgl1-mesa-dri needs to be reinstalled, but I can't find an archive for it.</code></pre></div>

<p>
	Yes&hellip; Thanks, I know&hellip; &ldquo;Sorry, we can't remove this package
	because we can't find a locally cached tarball to upgrade it, and
	apparently can't just redownload it from the repo&rdquo;&hellip;
</p>

<p>
	With my hope dwindling, I finally tried the old <code>install -f</code>,
	which works when you install a <code>deb</code> manually, which often
	requires you to temporarily break your system until you <code>install -f</code>.
	Installing a <code>deb</code> is an unfortunate process (that is slowly
	getting better with <code>apt</code> replacing <code>apt-get</code>) but that's not the topic for
	today's discussion.
</p>

<div class="code">
	<div class="code-title">~ % sudo apt install -f</div>
<pre><code>Reading package lists... Done
Building dependency tree
Reading state information... Done
E: The package libgl1-mesa-dri needs to be reinstalled, but I can't find an archive for it.</code></pre></div>

<p>
	Nope.
</p>

<p>
	My patience growing thin I finally resorted to copying and pasting the
	error into a search engine. StackOverflow, of course, delivered.
</p>

<div class="code">
	<div class="code-title">~ % sudo dpkg --remove --force-all libgl1-mesa-dri</div>
<pre><code> dpkg: libgl1-mesa-dri:amd64: dependency problems, but removing anyway as you requested:
 libglx-mesa0:amd64 depends on libgl1-mesa-dri.

 dpkg: warning: overriding problem because --force enabled:
 dpkg: warning: package is in a very bad inconsistent state; you should
  reinstall it before attempting a removal
  (Reading database ... 166745 files and directories currently installed.)
  Removing libgl1-mesa-dri:amd64 (18.2.8-0ubuntu0~18.04.2) ...</code></pre></div>

<p>
	<code>dpkg</code>, of course, complains, but does do as asked, and removes
	the broken package from the system. Finally.
</p>

<p>
	Now I can finally just run <code>apt install --fix-broken</code> which
	installed some various packages, their importance at this point not top of
	mind. At this point I am so tired of trying to fix the system I do not
	care.
</p>

<p>
	I can now run <code>apt</code> again, and running a subsequent <code>apt upgrade</code> was successful.
</p>

<hr>

<p>
	After all of this, I just cannot help but think about <code>dnf</code>. I
	have experienced failed upgrades in Fedora! It has happened! But because
	<code>dnf</code> gracefully knows a previous transaction was aborted it has
	facilities to resolve the situation, either through rolling back, retrying,
	or at the very least just letting you see up front what the state of your
	system is, without having to search for some arcane flags to a piece of
	software you never interact with (<code>dpkg</code>).
</p>

<p>
	The split-brain between <code>apt</code> and <code>dpkg</code> makes for a
	bit of a confusing mess, especially when the state gets out of sync between
	them, as was just chronicled in this post.
</p>

<p>
	The focus on this in recent years with <code>apt</code> is a glimmer of
	hope that this situation will improve in the Debian ecosystem. I work with
	Ubuntu systems regularly, and I always pine for <code>dnf</code> or
	<code>yum</code> most of the time. I sincerely hope Debian can catch up to
	where <code>dnf</code> has pulled ahead, and help one of the most
	widely-deployed operating systems in the world a little more stable and
	trustworthy.
</p>]]></content:encoded>
</item>
<item>
<title>Generating Static Sites with Make and Bash</title>
<link>https://www.alexblackie.com/articles/static-site-bash/</link>
<guid>https://www.alexblackie.com/articles/static-site-bash/</guid>
<pubDate>Sun, 16 Jun 2019 09:00:00 GMT</pubDate>
<author>alex@alexblackie.com (Alex Blackie)</author>
<description></description>
<content:encoded><![CDATA[ 		<p>
	When you think of <code>bash</code>, you may not consider it suitable for
	building websites. For the most part, you are probably correct. But I
	wanted to try anyway. I don't buy into most trends in the frontend web
	development community, so literally none of the "modern" static site
	generators appeal to me. All of them feel overcomplicated, too niche, or
	too javascript-y for my needs, which involve <em>just rendering a
	website</em>.
</p>

<p>
	Inspired by
	<a href="https://www.johnhawthorn.com/2018/01/this-website-is-a-makefile/">John Hawthorn's makefile website workflow</a>,
	I decided that was almost exactly what I wanted. However, I didn't really
	want to depend on anything outside of standard unix tools (so no Ruby).
	This is where bash comes in.
</p>

<h2>"uhhh"</h2>

<p>
	Look, hear me out. There are some reasons I actually ended up liking this:
</p>

<ul>
	<li>All the normal benefits of building static sites</li>
	<li>Really fast (sub-second full site builds on my laptop)</li>
	<li>No dependencies outside of GNU/POSIX tools</li>
	<li>It's kind of fun and weird</li>
	<li>i don't need to install 2500 npm packages and learn a cool new templating language that will be obsolete and unmaintained in 4 months</li>
</ul>

<p>
	Ultimately, this was just a fun project to make my personal site a little
	more interesting. This really isn't something that is overly practical.
	Nothing wrong with a little fun now and then, though!
</p>

<h2>The result</h2>

<p>
	Before we get in the weeds, let's see what the end-goal for project
	structure is. I've removed a few things that are in my own site, just to
	keep this example simple.
</p>

<div class="code">
	<div class="code-title">$ tree -F --dirsfirst</div>
<pre><code>.
â”œâ”€â”€ _build/
â”‚Â Â  â””â”€â”€ ...
â”œâ”€â”€ bin/
â”‚Â Â  â””â”€â”€ render*
â”œâ”€â”€ layouts/
â”‚Â Â  â””â”€â”€ site.html
â”œâ”€â”€ pages/
â”‚Â Â  â”œâ”€â”€ some-page/
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ index.html
â”‚Â Â  â”‚Â Â  â””â”€â”€ index.meta
â”‚Â Â  â”œâ”€â”€ index.html
â”‚Â Â  â””â”€â”€ index.meta
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ _/
â”‚   â”‚Â Â  â””â”€â”€ site.css
â”‚   â””â”€â”€ favicon.ico
â”œâ”€â”€ Makefile
â””â”€â”€ helpers.sh</code></pre>
</div>

<p>
	Basically, we have a directory for pages (<code>./pages</code>) and then one directory for each page, with a <code>.meta</code> file for metadata (title, variables, etc) and a <code>.html</code> file for the content itself.
</p>

<h2>Setting up <code>make</code></h2>

<p>
	Originally, I had started with pure bash -- just a single bash script that
	handled the entire pipeline from finding the files, to parsing them, to
	writing to a file. This actually worked out great, but was missing some
	niceties like incremental builds, and meant a lot of it was sort of
	"intertwined" where rendering a page had weird knowledge about where that
	page should be written to disk.
</p>

<p>
	So ultimately I decided simplifying the shell script to just render a page
	to <code>stdout</code> and let good ol&rsquo; <code>make</code> handle the
	reading and writing seemed like a more robust option.
</p>

<p>
	Luckily, because we're just running one command and writing that to disk,
	our Makefile can be pretty simple:
</p>

<div class="code">
	<div class="code-title">Makefile</div>
<div class="highlight"><pre><span></span><span class="c"># Find all html and meta files in the pages directory&lt;/span&gt;</span>
<span class="nv">pageSources</span><span class="o">=</span><span class="se">\$</span><span class="o">(</span>shell find pages -type f -name <span class="s1">&#39;*.html&#39;</span> -o -name <span class="s1">&#39;*.meta&#39;</span><span class="o">)</span>

<span class="c"># Map all the html files in the list of sources to the target path in _build</span>
<span class="nv">pageTargets</span><span class="o">=</span><span class="se">\$</span><span class="o">(</span>pageSources:pages/%.html<span class="o">=</span>_build/%.html<span class="o">)</span>

<span class="nf">all</span><span class="o">:</span> \<span class="k">$(</span><span class="nv">pageTargets</span><span class="k">)</span>

<span class="nf">_build/%.html</span><span class="o">:</span> <span class="n">pages</span>/%.<span class="n">html</span> <span class="n">pages</span>/%.<span class="n">meta</span>
<span class="c">	@# Silently (@) make all directories leading up to our target file (\$@)</span>
	@mkdir -p <span class="se">\$</span><span class="o">(</span>dir <span class="se">\$</span>@<span class="o">)</span>

<span class="c">	@# Run our render script, giving the source file name (\$&amp;lt;), substring-</span>
<span class="c">	@# replacing &quot;html&quot; with &quot;meta&quot; (as our script expects to be given</span>
<span class="c">	@# the meta file). Write the output to the target filename (\$@)</span>
	bin/render <span class="se">\$</span><span class="o">(</span><span class="p">&amp;</span>lt<span class="p">;</span>:html<span class="o">=</span>meta<span class="o">)</span> <span class="p">&amp;</span>gt<span class="p">;</span> <span class="se">\$</span>@

<span class="nf">clean</span><span class="o">:</span>
	rm -rf _build/*
</pre></div>
</div>

<p>
	All this does is find all <code>*.meta</code> and <code>*.html</code>
	files, and tell <code>make</code> that those should turn into a single HTML
	file in <code>_build</code>, by running the <code>bin/render</code> script.
	Of course, a lot of this is terse or a bit weird to read if you're
	unfamiliar with Makefiles, but in the end it is rather simple, and provides
	some pretty significant power for not a lot of code.
</p>

<h2>Proof of concept: straight renders</h2>

<p>
	To prove that this might actually work, the first thing to get going is a
	just rendering the page HTML itself, with basic variable substitution for
	things like page title, etc., but without a layout or any fancy stuff.
</p>

<p>
	The general idea here is to parse our template files inside a Here
	Document, which will give them full access to shell functions, local
	variables, and any command they want, really. This is obviously a huge
	security risk, but there should never be untrusted code being executed in
	this context, so the realistic risk is pretty minimal.
</p>

<div class="code">
	<div class="code-title">bin/render</div>
<div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env bash</span>

<span class="nb">set</span> -euf -o pipefail

render<span class="o">()</span> <span class="o">{</span>
	<span class="nv">title</span><span class="o">=</span><span class="s2">&quot;My web page&quot;</span>
	<span class="nb">echo</span> <span class="s2">&quot;</span><span class="k">$(</span><span class="nb">eval</span> <span class="s2">&quot;cat &lt;&lt;EOF</span>
<span class="s2">		</span><span class="k">$(</span>&lt;<span class="si">${</span><span class="nv">1</span><span class="p">/.meta/.html</span><span class="si">}</span><span class="k">)</span><span class="s2"></span>
<span class="s2">EOF&quot;</span><span class="k">)</span><span class="s2">&quot;</span>
<span class="o">}</span>

render <span class="nv">$@</span>
</pre></div>
</div>

<p>
	In essence, what we're doing here is reading an HTML file in as a string,
	which is interpolated into a heredoc, which is piped into <code>cat</code>,
	which is in a string, which we are evaluating as if it were a script
	itself. This means that the normal variable scope and expansion applies to
	our template.
</p>

<p>
	The <code>index.html</code> source file, for reference:
</p>

<div class="code">
	<div class="code-title">pages/index.html</div>
<pre><code>&lt;h1&gt;$title&lt;/h1&gt;</code></pre>
</div>

<p>
	And that's honestly about it. Everything else from here is just doing this
	multiple times, or adding some sugar or abstractions. Speaking of, let's
	make this nicer.
</p>

<h2>Adding a layout</h2>

<p>
	Currently we're just copying one file around, which isn't really helpful
	for a static site builder! What really made this "click" was getting a
	layout template wrapping the pages -- the most basic feature of any static
	site generator.
</p>

<p>
	So let's do that. All this is really doing is assigning the page to a
	variable, then rendering the layout as if it was the page.
</p>

<div class="code">
	<div class="code-title">bin/render</div>
<pre><code>#!/usr/bin/env bash

set -euf -o pipefail

render() {
	title="My web page"
<span class="code-rem">	echo "$(eval "cat &lt;&lt;EOF</span>
<span class="code-add">	content="$(eval "cat &lt;&lt;EOF</span>
		$(&lt;${1/.meta/.html})
EOF")"

<span class="code-add">	echo "$(eval "cat &lt;&lt;-EOF</span>
<span class="code-add">		$(&lt;layouts/site.html)</span>
<span class="code-add">	EOF")"</span>
}

render $@
</code></pre></div>

<p>
	In our layout, we can then just use <code>$content</code> where we want to
	render the page content.
</p>

<div class="code">
	<div class="code-title">layouts/site.html</div>
<pre><code>&lt;!doctype html&gt;
&lt;title&gt;$title&lt;/title&gt;
<span class="code-hl">$content</span></code></pre>

<p>
	We're getting pretty close to something useful. But hard-coding all our
	metadata in the build script doesn't scale. Let's fix that.
</p>


<h2>Metadata is just scripts</h2>

<p>
	Previously I had mentioned <code>.meta</code> files, but up until now we
	haven't used them.  Since our pages are being rendered within the context
	of the bash script, we can source other scripts in dynamically. So, our
	<code>.meta</code> files are just little bash scripts that are evaluated
	right before render. Thus, they could contain variables, functions, calls
	to external APIs or programs&hellip;
</p>

<p>
	Here's the <code>.meta</code> file for our page:
</p>

<div class="code">
	<div class="code-title">page/index.meta</div>
<pre><code>title="My web page"
lastUpdatedAt="2019-06-13"</code></pre>
</div>

<p>
	And in the build script, all we need to do is source our <code>.meta</code>
	file dynamically, as we do with the HTML template. Since our render script
	is being given the meta file as the first argument, we can just source
	that:
</p>

<div class="code">
	<div class="code-title">bin/render</div>
<pre><code>&hellip;
render() {
<span class="code-rem">	title="My web page"</span>
<span class="code-add">	. "$1"</span>
	content=$(eval "cat &lt;&lt;EOF
		$(&lt;${1/.meta/.html})
EOF")
&hellip;</code></pre>
</div>

<p>
	Another "lightbulb" moment is when you start to add shell functions. Want
	to format a date? Good news! Basically every computer probably already has
	<code>date(1)</code>.
</p>

<p>
	We can just add more functions to the <code>bin/render</code> file, or if
	we have a lot we can source a second file into it... It's just a shell
	script, so I mean you can do whatever you want.
</p>

<div class="code">
	<div class="code-title">bin/render</div>
<pre><code>#!/usr/bin/env bash

set -euf -o pipefail

<span class="code-add">formatDate() {</span>
<span class="code-add">	echo $(date -d "$1" +"%B %-d %Y")</span>
<span class="code-add">}</span>

render() {
&hellip;</code></pre>
</div>

<div class="code">
	<div class="code-title">pages/index.html</div>
<pre><code>&lt;h1&gt;$title&lt;/h1&gt;
Last updated: $(formatDate $lastUpdatedAt)</code></pre>
</div>

<p>
	That's it. Now we can use ISO-8601 dates in our metadata, but still display
	"human" dates on the site itself.
</p>

<p>
	You can even combine it with Here Documents to make more complex render
	helper functions. I do this on my site to dynamically discover blog
	articles from the filesystem, parse and sort them based on their date in
	metadata, and then render a provided template string with the new local
	variables. It's a bit convoluted and gross to use as a direct example, so
	here's a simplified one:
</p>

<div class="code">
	<div class="code-title">build.sh</div>
<pre><code>&hellip;

renderItems() {
	items=(one two three)
	template="$(while read template ; do echo $template ; done)"

	for item in ${items[*]} ; do
		eval "cat &lt;&lt;-EOFOR
			$template
		EOFOR"
	done
}

&hellip;</code></pre>

<div class="code">
	<div class="code-title">pages/index.html</div>
<pre><code>&hellip;

&lt;ul&gt;
$(renderItems &lt;&lt;-EOTEMPLATE
	&lt;li&gt;$item&lt;/li&gt;
EOTEMPLATE)
&lt;/ul&gt;</code></pre>
</div>

<p>
	At this point, that's basically 90% of a static site generator. You got
	layouts, templates, and metadata variables with support for helper
	functions, extenal "plugins" (normal CLI tools)&hellip; What more do you
	need?
</p>

<h2>Assets</h2>

<p>
	Every site will have some CSS, JS, images, and whatever else. You could
	call out to some preprocessor or crazy thing if you really wanted, but for
	me I am happy with raw CSS and JS, so here is my asset pipeline:
</p>

<div class="code">
	<div class="code-title">Makefile</div>
<pre><code>pageSources=$(shell find pages -type f -name '*.html' -o -name '*.meta')
pageTargets=$(pageSources:pages/%.html=_build/%.html)

<span class="code-add">staticSources=$(shell find static -type f)</span>
<span class="code-add">staticTargets=$(staticSources:static/%=_build/%)</span>

<span class="code-rem">all: $(pageTargets)</span>
<span class="code-add">all: $(pageTargets) $(staticTargets)</span>

&hellip;

<span class="code-add">_build/%: static/%</span>
<span class="code-add">	@mkdir -p $(dir $@)</span>
<span class="code-add">	cp $< $@</span>

&hellip;
</code></pre>

<p>
	ðŸ–ŒðŸ‘Œâœ¨
</p>

<h2>What's the catch?</h2>

<p>
	While the core idea proved to work out great, it was not without rough
	edges. In the end, these all proved fine to work around or put up with, but
	they exist nonetheless.
</p>

<ul>
	<li><strong>Escaping normal content</strong> &mdash; since every page is parsed as a string, you need to be careful what that string contains. For example, "<code>I paid $50</code>" needs to be "<code>I paid \$50</code>" to avoid <code>$5</code> being parsed as a variable.</li>
		<li><strong>Shell scripts kind of suck</strong> &mdash; shell scripting is not really a pleasant environment. Decades of weird decisions, cumbersome syntax... While shell scripts get you a powerful and highly extensible environment, that comes at a cost of some readability and "niceness" that you might expect from something like ERB or Handlebars, for example.</li>
		<li><strong>Cross-OS compatibility is a nightmare</strong> &mdash; you'd think POSIX meant everything behaves the same but HA-HA NOPE good luck maintaining weird <code>if</code> statements to try and make sure you pass different flags to GNU vs. BSD <code>date</code> or <code>find</code> to support both platforms.</li>
</ul>

<h2>Also I left out a bunch of other stuff</h2>

<p>
	I have left out a significant amount of work and numerous features that are
	implemented in the "real" version of this script that powers my site. You
	are free to
	<a href="https://github.com/alexblackie/watashi">peruse my website's source code</a>,
	if you are curious. This post was intended not to be a guide or tutorial
	but just a neat showcase of a semi-crazy idea.
</p>

<p>
	In brief, some features that were pretty easy to add that my script ended
	up supporting:

	<ul>
		<li>custom layouts per page</li>
		<li>nested layouts for articles</li>
		<li>xml pages and layouts (rss feed)</li>
		<li>dynamically building list of articles from filesystem</li>
	</ul>
</p>

<hr>

<p>
	This was a fun project for me, and the end result solved a real need.
	Hopefully it was as interesting to you as it was me!
</p>]]></content:encoded>
</item>
<item>
<title>DevOps in the Closet</title>
<link>https://www.alexblackie.com/articles/devops-in-the-closet/</link>
<guid>https://www.alexblackie.com/articles/devops-in-the-closet/</guid>
<pubDate>Mon, 18 Jun 2018 09:00:00 GMT</pubDate>
<author>alex@alexblackie.com (Alex Blackie)</author>
<description></description>
<content:encoded><![CDATA[ 		<p>
  I love hardware. I love the raw power you can get from bare metal, and the
  ownership and control one can have over it. Even though my needs are modest,
  and could easily be satisfied by a couple mid-range VPS instances, there's no
  fun in that! Recently I moved to a hybrid setup -- a 12U rack of machines in
  my apartment connected to a load balancer running on a cloud provider.
</p>

<p>
  For a while I rented colocation space from the wonderful people at Hurricane
  Electric -- I highly recommend checking them out if you need colocation, IP
  transit, or anything of the sort. However, in my efforts to move all my data
  and property within Canadian borders, I had to take my children back into my
  home.
</p>

<p>
  I am fortunate enough to have a symmetrical gigabit fibre connection to my
  apartment, and am doubly fortunate to have the resources and ability to run
  it <a href="/articles/fibe">entirely on my own hardware</a> &mdash; this
  means I have ample bandwidth to spare, and with a newfound pile of servers at
  my disposal, what better than to run production traffic from my closet?
</p>

<h2>Overcoming NAT</h2>

<p>
  The first problem here, of course, is addressing. Consumer ISPs being what
  they are, my residential internet plan has only IPv4, and only one address.
  Not only that, but an address that is different every time the PPPoE tunnel
  renegotiates. None of these attributes are particularly helpful for hosting.
</p>

<p>
  But then, an idea: what if I used a VPN <em>as they are intended to be
  used</em>? Everyone loves to use VPNs these days just to tunnel their traffic
  through someone else's connection, but really that's nothing more than an
  abuse of a very powerful tool. The point of a Virtual Private Network server
  is, well, to provide a virtual, private network.
</p>

<p>
  <strong>So I set up a VPS running OpenVPN</strong>. Every machine in my
  closet has an OpenVPN client running on it, connected to this server (I refer
  to this machine as "the hub"). All machines connect to this VPN and get a
  static IP assigned to them in the VPN's private subnet. Now, any machine
  connected to The Hub can seamlessly communicate with any other, as if they
  were sitting next to each other on a switch. <strong>This</strong> is what
  VPN's are for.
</p>

<p>
  The astute among you may have already realised: there's nothing about this
  setup that restricts it to servers in my closet. Indeed, since the OpenVPN
  Clients authenticate using certificates, I could simply copy the certs to a
  server somewhere entirely different, and it would connect and be available on
  the same IP. This means with no deployments or config changes I can fail-over
  machines to ones in entirely different regions, behind entirely different
  networks -- even a hidden machine in my parents' basement -- and it would be
  available just the same as everything else, and on the same IP.
</p>

<p>
  <strong>The one downside</strong> with this setup is the OpenVPN "hub" server
  is a single point of failure. If the VPN goes down, so does every machine
  connected to it. I'm sure there are ways to work around this and deploy a HA
  VPN, but for my purposes I'm fine with the risk of failure in the name of
  simplicity.
</p>

<h2>Serving the Web from my closet</h2>

<p>
  So thus far I have a bunch of servers connected to a VPN and it's all peachy,
  but this still doesn't solve the original problem: they're still not
  publicly-accessible. A private IP, even if connected to a public server, is
  still private. <strong>Enter, HAProxy.</strong>
</p>

<p>
  Playing off of the fact any machine can connect to the VPN, I have a second
  VPS beside The Hub running HAProxy. This machine acts as my "edge" (the
  machine that accepts all incoming traffic) and has IPv6, backups, a fast
  datacenter-grade network connection, etc. This machine is also connected to
  the VPN, just as the rest of the servers, and HAProxy's backends point to the
  static IPs on the VPN for the respective services.
</p>

<p>
  This allows me to still support IPv6 and still have the edge of my network in
  a reliable place, but host the bulk of my data and harness the power of raw
  hardware at the same time -- without having to shell out hundreds of dollars
  per month to pay for colocation.
</p>

<p>
  Additionally, this allows me to build in high-availability completely
  transparently -- I could have HAProxy load balance between two servers: one
  in my closet, and one on a VPS provider (for example), and then even if my
  power goes out or I accidentally unplug everything while cleaning traffic
  would be uninterrupted. This also means that under heavy traffic, I can
  quickly scale-out instances on a cloud provider to handle it, while still
  keeping my affordable baseline of dedicated hardware.
</p>

<h2>Simplified management</h2>

<p>
  One other benefit of this setup is <strong>my laptop has an IP on The Hub
    too</strong>. With one click I can have my laptop directly attached to the
  production VPN, and have full access to every machine in my arsenal, no
  matter where it is or what firewalls it may be behind.
</p>

<p>
  This means I don't have to expose SSH anywhere except on the private VPN
  network (as I can run Ansible anywhere in the world over the VPN), and don't
  need to worry about which servers are on which providers to make sure I have
  the right VPNs enabled -- it's only one VPN everywhere.
</p>

<h2>Utilization of&hellip; unusual hardware</h2>

<figure>
  <img src="https://cdn.blackie.zone/alexblackie/devops-closet/unconventional-desires.jpg" alt="A top-down shot of a rack shelf with some consumer-grade computers" width="960" height="342">
</figure>

<p>
  In addition to my pile of Xeon-based production-grade server hardware, I also
  had a pile of unused consumer trash boxes that needed something to do.  Under
  this new setup, I now am running several "internal" or "secondary" servers on
  a few perhaps-not-production-grade computers: a Gigabyte Brix and a Zotac
  ZBox, as well as a couple old laptops.
</p>

<p>
  These are obviously not machines I would have put in a datacenter, but are
  still useful nonetheless, and for non-critical "tier two" services (like CI
  builders, yum repo mirrors, or internal services) they serve their role
  perfectly and compliment the "shoestring budget" of the entire setup.
</p>

<p>
  This has made me feel much better about the previously-useless pile of old
  laptops and dust-covered relics I had been hiding away in a box. This project
  has allowed me to give them a new life and purpose.
</p>

<h2>But Don't Do This.</h2>

<p>
  This setup works great for me, because I run a handful of shitty websites
  only for myself, and don't get any significant amount of traffic.
  <strong>Please do not use this article as a guide. Do not deploy this
  ever.</strong> If any of my projects ever came close to hosting anything that
  mattered or held customer data I would immediately move everything to
  colocation and implement full high-availability practices.
</p>

<p>
  This was a fun project, and since I pay out-of-pocket for all this it made a
  significant budgeting difference. If you're in a similar boat, perhaps it
  could work for you, but absolutely do not think this is an acceptible
  production network for anything that remotely matters.
</p>

<h2>What would it take&hellip;</h2>

<p>
  But <em>what if</em> you did in fact want to make this setup more legitimate?
  What if we wanted to deploy a proper on-premises "hybrid cloud" solution?
  Perhaps surprisingly, it's not that far off from what we already have.
</p>

<ol>
  <li>
    <p><strong>Router-level tunneling</strong>.</p>
    <p>
      Switching the individually-connected servers to a router-level network
      bridge would probably be a better permanent solution. This would mean
      running a single tunnel between the on-prem VLAN and the remote cloud
      private network, and continuing to utilize the existing subnets of both.
      This would mean every machine in each subnet would be automatically
      visible to the whole network, as the tunnel is bridging <em>networks</em>
      instead of <em>machines</em>.
    </p>
    <p>
      However, this would impose more conditions on each on-prem network: each
      network being connected would have to be specifically designed to be
      connected. This would lose the current benefit of being able to connect
      any machine anywhere without anything more than running OpenVPN.
    </p>
    <p>
      This also would mean using a simple VPS for this would probably not work,
      as the networking options provided by most VPS providers are limited at
      best. You would need something more akin to an AWS VPC or Azure Virtual
      Network.
    </p>
  </li>
  <li>
    <p><strong>HA OpenVPN</strong>.</p>
    <p>
      The OpenVPN "Hub" server is a single point of failure in this design. For
      more legitimate hybrid cloud deployments, you'd want to ensure high-availability
      of the OpenVPN hub so that a machine failure or region outage wouldn't
      take down the entirety of your infrastructure.
    </p>
    <p>
      Unfortunately I don't really know the extent of OpenVPN's HA capabilities,
      as it's never been something I've needed; the first thought I have is CARP
      to enable machine-level redundancy in case of hardware failure. This isn't
      "globally-redundant", but it is a good first step. Chances are, if you're
      running on-prem, an outage of that premises would be catastrophic anyway.
    </p>
  </li>
</ol>]]></content:encoded>
</item>
<item>
<title>A Review of the Dell Latitude 7214</title>
<link>https://www.alexblackie.com/articles/dell-7214/</link>
<guid>https://www.alexblackie.com/articles/dell-7214/</guid>
<pubDate>Sun, 06 May 2018 09:00:00 GMT</pubDate>
<author>alex@alexblackie.com (Alex Blackie)</author>
<description></description>
<content:encoded><![CDATA[ 		<a class="scrollAnchor" name="intro"></a>
<p>
	For many years now, Lenovo Thinkpads have been a mainstay in my life; their
	no-nonsense, practical offering greatly appealed to what I wanted in a
	machine. Its shortcomings in weight, thickness, and aesthetic were all areas
	I was more than happy to trade for superb battery life, port abundance, and
	"in the field" features like Power Bridge Batteries.
</p>

<p>
	However, Lenovo has been slowly turning Thinkpads into mainstream consumer
	trash. I found myself in want of a new laptop recently and found the entirety
	of the modern Thinkpad lineup revolting. The X280 is a disgrace to its
	namesake, the X1 Carbon makes way too many vain compromises for its thin
	profile, and everything else was either too big or too absurd for my needs
	(eg., the P71 fits both of those descriptors). I found myself at a loss,
	unsure of where to turn now that my beloved Thinkpads were spoiled by
	marketing and consumer trends.
</p>

<p>
	Recently I have been watching (all too frequently, but that is besides the
	point) Stargate Atlantis, and being the type of person I am took note
	immediately of their excellent use of various "rugged" or "Toughbook-style"
	laptops. Appropriate for toting off-world! This reminded me of the "rugged"
	laptop category, and thus began my research: could I find a small and
	relatively portable rugged laptop for a reasonable price?
</p>

<p>
	The answer is no.
</p>

<a class="scrollAnchor" name="landscape"></a>
<h2>The Rugged Landscape</h2>

<p>
	I started my journey with the classic <strong>Panasonic Toughbook</strong>
	line, but immediately discovered they look like children's toy computers from
	2006. I'm not one to quibble over aesthetics but I found the Panasonic
	Toughbook series just simply repulsive. Not to mention the fact they would
	have set me back upwards of $5000 CAD, which I was absolutely not comfortable
	with committing for this little experiment.
</p>

<p>
	I started digging a bit more and discovered what could be my new favourite
	company: <a href="http://en.getac.com/notebooks/index.html">Getac</a>. I
	immediately was drawn to their absurd "rugged laptop + a 1U server bolted to
	the bottom" product, and knew I was going in the right direction.
	Unfortunately, while the V110 looked like exactly what I was after, it's
	nearly impossible to find through normal channels in Canada. I did contact a
	local distributor for a quote but never followed up, as, while not as bad as
	a Toughbook, I was over-reaching my budget fairly handsomely.
</p>

<p>
	And then I found it: <a href="http://www.dell.com/en-ca/work/shop/laptops-ultrabooks/latitude-12-rugged-extreme-convertible-laptop/spd/latitude-12-7214-2-in-1-laptop/xctol721412usca">The Dell Latitude 12 "Rugged Extreme" (7214)</a>.
	I dug up some photos and reviews from around the web, and every part of this
	laptop spoke to me. The design is on-point, the featureset is plentiful, the
	chipset is modern, it's highly configurable... Except it <em>starts</em> at
	~$6200 CAD.
</p>

<p>
	At this point, I was about to give up. It was a fun idea, but these prices
	are just too high for an individual like me to justify, at least for the time
	being. I'm not going to drop upwards of $6k on a laptop I might not even
	like, and can't find more than a couple reviews of.
</p>

<a class="scrollAnchor" name="ebay"></a>
<h2>Voyage to the Electronic Bay</h2>

<p>
	I have perused eBay many times in the past, but never built up enough courage
	(or money) to follow through and purchase something. I ended up on the site
	through a web search while researching the Dell 7214, and said, "oh, well of
	course!"
</p>

<p>
	Rugged laptops aren't popular in the mass market, for whatever reason.
	They're usually purchased by large organizations for short-term projects, and
	then liquidated at the end to refurbishing centres or what-have-you. This
	means you can find them on-line for <em>steep</em> discounts.
</p>

<p>
	The machine I'm typing this on now has an Intel Core i7-6600U, 32GB RAM, and
	a 256GB M.2 (SATA) SSD. After shipping, USD &rarr; CAD conversion, taxes, and
	import fees: a hair over $1500. Well under what I was expecting to pay for an
	X1 Carbon, and with a whole hell of a lot more horsepower and ports. It's
	perhaps worth noting I replicated my configuration on Dell's website and it
	totalled a little north of $10k CAD. Ouch.
</p>

<p>
	Unfortunately, eBay.
</p>

<p>
	It took a full 8 business days to get from Michigan to QuÃ©bec, travelling in
	a circle more than once on its way here, and taking a nice 3-day rest in the
	receiving dock of the customs brokerage. It was one of the most frustrating
	package tracking experiences I've had, and until I got the "out for delivery"
	notification I was starting to wonder if I'd ever see it.
</p>

<p>
	Once it did arrive, however, I was ecstatic.
</p>

<a class="scrollAnchor" name="hazards"></a>
<h2>Hazards not included</h2>

<figure>
	<img src="https://cdn.blackie.zone/alexblackie/dell-7214/rugged.jpg" alt="Laptop with hardware scattered on top." width="960" height="387">
</figure>

<p>
Just how "rugged" is this machine? According to <a href="http://i.dell.com/sites/doccontent/shared-content/data-sheets/en/Documents/dell_latitude_12_rugged_extreme_spec_sheet.pdf">the whitepaper [PDF]</a>,
it conforms to the "MIL-STD-810G" testing specifications, which are:
</p>

<blockquote>
	Transit drop (72",60",48"; single unit; 78 drops), operating drop (36"),
	blowing rain, blowing dust, blowing sand, vibration, functional shock,
	humidity, salt fog (with rubberized keyboard), altitude, explosive
	atmosphere, solar radiation, thermal extremes, thermal shock, freeze/thaw,
	tactical standby to operational
</blockquote>

<p>
	Additionally,
</p>

<ul>
	<li>IP65 rating for particulates and liquid</li>
	<li>Fully operational between -29&deg;C and 63&deg;C</li>
	<li>MIL-STD-461F electromagnetic interference certification (I didn't bother looking this one up)</li>
</ul>

<p>
	... and a bunch more. You hopefully get the idea. I can't forsee myself ever
	being in a situation remotely close to any of these limits, but it's good to
	know I can work outside or in the rain without panicking or damaging
	something.
</p>

<a class="scrollAnchor" name="bad"></a>
<h2>The Bad</h2>

<a class="scrollAnchor" name="weight" data-parent="bad"></a>
<h3>Weight</h3>

<p>
	Make no mistake, this is a <strong>heavy</strong> laptop. All of these extra
	materials, mechanisms, and flourishes that make it "rugged" all also add
	weight. It clocks in at around 3kg, which is heavier than my 14" Thinkpad
	T460p and its extra-large battery by about 400g or-so.
</p>

<p>
	Mine does have the carry-handle addon, which makes it very easy to carry
	around wherever I need to go, but put it in a messenger bag and you're
	shoulder might start to ache. It's certainly not the <em>heaviest</em> laptop
	I've seen, but it's also certainly not lightweight.
</p>

<a class="scrollAnchor" name="display" data-parent="bad"></a>
<h3>The Display</h3>

<figure>
	<img src="https://cdn.blackie.zone/alexblackie/dell-7214/screen.jpg" alt="The screen in profile" width="960" height="380">
</figure>

<p>
	This machine doesn't have extensive screen options. No matter what, you get a
	11.6" 1366x768 TN panel with an anti-glare coating and resistive touchscreen.
</p>

<p>
	I don't mind the resolution <em>too much</em> at this size; I think 11.6" is
	small enough that such a measly resolution isn't out-of-place. That said,
	something <em>slightly</em> higher (like 1600x900), or even just an IPS-type
	panel would be a very welcome change.
</p>

<p>
	The anti-glare coating works quite well; Dell describe it as an
	"outdoor-viewable" screen, and I would have to agree. The coating mixed with
	the insanely bright backlight (I currently am working indoors and have it at
	about 20%, and it's as bright as my System76 Galago at 100%) make for a
	screen that truly "works anywhere". However the polarization and resistive
	layer do add a bit of a "depth" effect where it looks like there's a bit of
	space between my fingerprints and the screen below.
</p>

<p>
	The touchscreen is medicore as well. I mentioned it's resistive, which means
	it works based on pressure, so you can use it in the rain and/or with a stick
	you found in the woods and it'll work perfectly fine. It does mean, however,
	that it has that "old school" touchscreen behaviour where touches take a bit
	longer to register, aren't particularly accurate with a finger, and don't
	always work if you're used to capacitive screens and just caress the surface
	with your fingertip.
</p>

<figure>
	<img src="https://cdn.blackie.zone/alexblackie/dell-7214/testufo.jpg" alt="Photo of testufo.com test result" width="960" height="380">
</figure>

<p>
	One thing I have noticed is serious ghosting issues. I've tried to capture it
	in the above photo with the <a href="https://testufo.com/">testufo.com</a>
	tests. It's not entirely noticable in general use, but now and then you can
	catch it while scrolling web page with large text, or quickly-moving objects.
	For what I do, which mostly involves static screens full of text, it's rarely
	a noticeable issue.
</p>

<p>
	With all that said, I still give the screen a "pass". It's not unusable by
	any stretch of the imagination, it's just not <em>great</em>. We've been
	spoiled in the last few years by HiDPI screens and incredibly responsive
	capacitive touchscreens that using this one feels like a step into the recent
	past. But given some time to adjust, you can learn its idiosyncracies and
	it's just fine.
</p>

<a class="scrollAnchor" name="touchpad" data-parent="bad"></a>
<h3>Touchpad</h3>

<p>
	I will not weasel my way around the trackpad &mdash; <strong>it's
	  bad.</strong> So bad that I avoid using if I can and move to the
	touchscreen. It's also resistive, like the touchscreen, which means you kind
	of need to press your fingers into it for it to activate. Once you do, you'll
	notice it's fairly small and pretty inaccurate.
</p>

<p>
	Small, inaccurate, hard to activate, and just overall unpleasant. It's
	probably the biggest downside I've experienced so far with this machine. I'm
	very glad the touchscreen is reasonable enough to make up for it.
</p>

<a class="scrollAnchor" name="handle" data-parent="bad"></a>
<h3>The Handle</h3>

<figure>
	<img src="https://cdn.blackie.zone/alexblackie/dell-7214/handle.jpg" alt="Top-down shot of the handle" width="960" height="270">
</figure>

<p>
	If the sheer thickness of the laptop doesn't catch your eye, it's chunky
	built-in handle certainly will.
</p>

<p>
	The handle sticks out the front, and is slightly higher than the keyboard
	deck. This means if the surface you're using is slightly too high, the handle
	will press into your arms. It's a nice, rubberized coating so it doesn't hurt
	or anything like that, but it is kind of annoying.
</p>

<p>
	But surely you can remove the handle, right? Right! Well&hellip; The handle
	acts as a giant antenna for all (four?) of the radios in this machine. That
	means while it looks like you can just unscrew it and take it off, in fact
	you have to entirely disassemble the laptop to disconenct all the antenna
	leads.
</p>

<p>
	Not only that, but then once you remove the handle, there will be exposed
	electronics on the exterior of the laptop, as you can't just buy more rubber
	nubs for the corners to replace where the handle was mounted.
</p>

<p>
	So without purchasing a <em>second</em> one of these to scavenge parts off
	of, removing the handle is not really an option; and even if I did have the
	replacement nubs, the amount of time and work involved might not even be
	worth it.
</p>

<p>
	All that said, the handle is actually really convenient for carrying the
	machine around. The rugged exterior does mean that you can take it to a
	coffeeshop without a sleeve, just carrying it by the handle. But it would be
	nice if I could more easily disconnect it for when it's just sitting on a
	desk.
</p>

<a class="scrollAnchor" name="good"></a>
<h2>The Good</h2>

<a class="scrollAnchor" name="performance" data-parent="good"></a>
<h3>Performance</h3>

<p>
	Often "rugged"-style machines sacrifice performance because of thermal or
	power limitations -- this is not the case at all here.
</p>

<p>
	<strong>The CPU</strong> is an IntelÂ® Coreâ„¢ i7-6600U @ 2.6GHz and with 4KB of
	L3 cache. Even though it's only a U-class CPU, so far the performance has
	been quite impressive, and is more than enough for everything I need to do.
	For some sort of benchmark, I had it calculate all prime numbers up to
	100,000 using all 4 of its threads:
</p>

<div class="code">
	<div class="code-title">sysbench cpu --cpu-max-prime=100000 run --threads=4</div>
	<pre>
sysbench 1.0.12 (using system LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 4
Initializing random number generator from current time


Prime numbers limit: 100000

Initializing worker threads...

Threads started!

CPU speed:
	  events per second:   144.34

General statistics:
	  total time:                          10.0233s
	  total number of events:              1447

Latency (ms):
	       min:                                 22.90
	       avg:                                 27.67
	       max:                                 50.58
	       95th percentile:                     28.67
	       sum:                              40044.87

Threads fairness:
	  events (avg/stddev):           361.7500/1.64
	  execution time (avg/stddev):   10.0112/0.01</pre>
</div>

<p>
	<strong>The disk</strong> is an M.2 SATA SSD. The specific disk I have is a
	256GB Sandisk X400. It's an entirely average, consumer-grade SSD from what I
	can measure. A quick <code>fio</code> benchmark shows mediocre performance:
</p>

<div class="code">
	<div class="code-title">fio --name=test</div>
<pre>
test: (groupid=0, jobs=1): err= 0: pid=20038: Sat May  5 12:18:38 2018
	 read: IOPS=51.1k, BW=200MiB/s (209MB/s)(4096MiB/20511msec)
[...]</pre>
</div>

<p>
	Nothing really spectacular, but still better than any spinning rust. However,
	a more intense random IO test really makes the drive choke.
</p>

<div class="code">
	<div class="code-title">fio --randrepeat=1 --size=2G --readwrite=randrw --rwmixread=75 --name=test</div>
<pre>
test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=psync, iodepth=1
fio-3.3
Starting 1 process
Jobs: 1 (f=1): [m(1)][99.0%][r=18.0MiB/s,w=6178KiB/s][r=4618,w=1544 IOPS][eta 00m:01s]
test: (groupid=0, jobs=1): err= 0: pid=19831: Sat May  5 12:09:55 2018
	 read: IOPS=4056, BW=15.8MiB/s (16.6MB/s)(1534MiB/96821msec)
	 [...]
	 write: IOPS=1359, BW=5436KiB/s (5566kB/s)(514MiB/96821msec)
	 [...]
</pre>
</div>

<p>
	Yikes. Fow now it'll do fine (especially with NAND prices where they
	currently are), but in the future I will likely swap this out for something a
	little higher-end.
</p>

<p>
	<strong>The RAM</strong> is 32GB DDR4 in dual-channel. The sticks I got
	appear to be Samsung OEM modules at 2133MT/s. This is my first non-server
	machine to have &gt;16GB RAM and I'm still unsure what to even do with all of
	it (oh wait, I just opened Slack; never mind: it's all used).
</p>

<a class="scrollAnchor" name="keyboard" data-parent="good"></a>
<h3>Keyboard</h3>

<figure>
	<img src="https://cdn.blackie.zone/alexblackie/dell-7214/keyboard.jpg" alt="Dell 7214 Keyboard with blue backlight" width="960" height="261">
</figure>

<p>
	I was somewhat concerned about the keyboard before using it. Would it be made
	mushy and unpleasant in the name of "ruggedness"? No reviews I could find
	mentioned anything about it, so it was a bit of a gamble. Luckily, all my
	worrying was once again proved unfounded; this keyboard is
	<em>excellent</em>.
</p>

<p>
	It definitely feels different that any other laptop keyboard I've used. You
	can almost hear the rubber protection underneath when typing, but somehow the
	keys maintain a wonderfully crisp and responsive bump, landing in the soft
	embrace of the aforementioned rubber. This makes for a near-silent keypress,
	leaving only the noise of the bounce-back of the spacebar audible.
</p>

<p>
	The key size is slightly smaller than on my other, larger laptops, but I find
	my typing is just as accurate and quick as any other keyboard (perhaps even
	moreso).
</p>

<p>
	Overall, I find typing on this keyboard very enjoyable, easily equal to the
	experience of a Thinkpad or Topre external keyboard. Dell did an excellent
	job keeping it pleasant while also making it "rugged".
</p>

<p>
	This of course is all overshadowed by the fact it has RGB backlighting. It
	can cycle between green, blue, whiteish, and red with a Fn key combination
	("whiteish" as it has the classic RGB LED problem where it looks more like a
	light purple).
</p>

<p>
	The BIOS provides customization for the backlighting, including providing
	custom RGB values for 2 user presets, and changing the default backlight
	colour on boot. I didn't really bother to set my own custom colours, but did
	change the default to red, as it is the least jarring in the dark (the only
	situation in which I use backlighting).
</p>

<a class="scrollAnchor" name="thermals" data-parent="good"></a>
<h3>Thermals</h3>

<figure>
	<img src="https://cdn.blackie.zone/alexblackie/dell-7214/load-test.png" alt="htop showing load" width="960" height="200">
</figure>

<p>
	I'm not usually impressed by laptop thermal solutions because usually they're
	"adequate". It's a fan, it spins, and when you push the CPU it gets audible.
	This is not the case with this laptop, however.
</p>

<p>
	I noticed on the bottom of the laptop there is a small vent with a reasonably
	small laptop fan behind it. I also noticed that while the computer was on,
	this fan was standing still. So I fired up <code>stress-ng</code> to put some
	load on the CPU (100% load on all cores, to be exact). The experiment went
	something like this&hellip;
</p>

<p>
	I'm sitting in a large room with an ambient temperature of around 24&deg;C.
	At idle, the CPU sits around 40&deg;C with the fan entirely off. Under 100%
	load on all cores, the CPU temperature slowly climbs over the course of about
	three minutes to 82&deg;C and holds &mdash; the passive cooling manages to
	keep the CPU under its 100&deg;C limit. After four minutes, the fan finally
	kicks in to ~3100RPM and the temperature rapidly drops and holds around
	67&deg;C.
</p>

<p>
	I ran the stress test for several minutes and at no point did the top part of
	the laptop deck get warm, and only for the last few minutes did the fan even
	spin. Dell calls their thermal solution "QuadCool&trade;", but never explains
	what that means; all I know is&mdash;whatever the magic is&mdash;it works
	really damn well.
</p>

<a class="scrollAnchor" name="battery" data-parent="good"></a>
<h3>Battery and Power</h3>

<figure>
	<img src="https://cdn.blackie.zone/alexblackie/dell-7214/battery.png" alt="Screenshot of GNOME Power settings" width="960" height="151">
</figure>

<p>
	Another strong point of this machine is its battery. 56Wh is not a large
	capacity, but it's removable and replacements are easy to purchase, so you
	can carry an extra in your backpack while travelling. In my experience, doing
	"light" tasks like work over SSH, screen at half brightness, and a bit of web
	browsing, I can expect around <strong>8 hours</strong> per battery. If you
	kill the brightness, keyboard backlight, and radios, you could probably
	manage to squeeze a couple more hours out of it.
</p>

<p>
	With two batteries, that means you can get 14 hours of medium-light usage
	without even trying (and assuming shutting down to swap the batteries is
	acceptable). This rivals even my old X250, which had much worse performance.
</p>

<a class="scrollAnchor" name="tablet" data-parent="good"></a>
<h3>Convertible/Tablet mode</h3>

<figure>
	<img src="https://cdn.blackie.zone/alexblackie/dell-7214/tablet.jpg" alt="Laptop in tablet mode." width="960" height="400">
</figure>

<p>
	This is my first-ever experience with a "convertible" laptop. I've actively
	avoided such laptops in the past because I strongly dislike their
	"tent-style" flip-over hinges. Why would I want the bottom to be the
	keyboard?? Disgusting. The Dell 7214 has a much better solution.
</p>

<figure class="pull-right">
	<img src="https://cdn.blackie.zone/alexblackie/dell-7214/flippy.jpg" alt="Laptop screen mid-flip." width="403" height="350">
</figure>

<p>
	The screen is actually two parts: an outer frame and an inner screen housing.
	The inner part swivels around on its midpoint, allowing you to click down the
	lid and have a tablet-like experience without any of the nasty exposed
	keyboard or weird hinges. Note my use of "tablet-like", as this is still a
	3kg machine, so holding it comfortably with one hand is a long-shot.
</p>

<p>
	I find myself using this functionality much more than I thought. It's
	incredibly nice to open a long article, flip the screen and read it in
	portrait mode on the couch.
</p>

<p>
	And, as is inevitable, I have begun poking at my work laptop's screen
	involuntarily, forgetting it is just a normal non-touch screen.
</p>

<a class="scrollAnchor" name="linux" data-parent="good"></a>
<h3>Linux Support</h3>

<p>
	I was pleasantly surprised to find that almost all parts of this laptop are
	compatible out-of-the-box with modern Linux. GPS, LTE modem, touchscreen,
	rotation accelerometer&hellip; Everything worked out-of-the-box (at least on
	Fedora 28) and without any intervention from myself. The only exception is
	the fingerprint reader, but I have not had one of those work under Linux in
	years.
</p>

<p>
	GNOME, specifically, has been an absolute pleasure. I've used GNOME for many
	years on "normal" laptops, but I'm consistently surprised by just how well it
	handles the "convertible" features: the on-screen keyboard is just a swipe-up
	away, rotation lock buttons appear in the system menu, basically every touch
	target is plenty large for my stubby fingers&hellip; If anything has renewed
	my faith in GNOME, it has been my experiences since using it on this machine.
</p>

<a class="scrollAnchor" name="anywhere" data-parent="good"></a>
<h3>Working from Anywhere</h3>

<p>
	I was glad to see that the model I bought came with a built-in LTE modem and
	dedicated GPS module. The SIM slot takes a MicroSIM-sized card, so
	unfortunately none of mine would fit properly, and thus I haven't had a
	chance to test it yet.
</p>

<p>
	I've always loved the idea of having LTE connectivity in my laptop. Thinkpads
	have had the option for a long while, but I never opted to pay for the
	upgrade. I love the idea that no matter where I am in the world, I can get a
	prepaid or pay-as-you-go data SIM, pop it into my laptop, and never be
	without an internet connection. This is obviously great for travelling, but
	also just if I end up in a coffeeshop that doesn't have WiFi.
</p>

<a class="scrollAnchor" name="slots" data-parent="good"></a>
<h3>Too many slots</h3>

<p>
	This is the first machine I've ever owned that has an ExpressCard slot, and
	as of now I have no idea what to use it for. It seems the best option is to
	get a card with a pair of USB3 ports on it, expanding the weak USB offering
	of the machine.
</p>

<p>
	Additionally, this is the first machine I've owned with a smartcard reader.
	This takes two forms: there's a contactless reader in the palmrest, and a
	full smartcard slot underneath the ExpressCard bay. Similar to the
	ExpressCard slot, I'm unsure how to take advantage of this new functionality;
	a quick search shows that smartcards are difficult to buy for quantities
	under 100, and open source compatibility isn't always guaranteed.
</p>

<p>
	I also fear if I come to love both of these features, I will be disappointed
	in the future when they are inevitably impossible to find in newer laptops.
	Best to enjoy it while it lasts, I guess.
</p>

<a class="scrollAnchor" name="brief"></a>
<h2>In Brief</h2>

<p>
	The Dell Latitude 7214 "Rugged Extreme" is a lot of laptop for one person to
	handle. It's heavy, bulky, thick, and makes a lot of compromises to achieve
	many of its certifications and goals. However, if you are willing to accept
	these compromises, you get a machine with one hell of a spec sheet.
</p>

<p>
	This is not a machine I would generally recommend. To enjoy this machine, you
	need to have rather peculiar desires and be willing to go out of your way to
	satisfy them. If you're willing to compromise on the weight, thickness,
	display quality, and trackpad for a machine that will probably survive in
	environments a human cannot, the Dell Latitude 7214 certainly delivers.
</p>

<p>
	Did I mention it has RGB?
</p>

<hr>

<p>
	<small>
	  Desktop background pictured is <a href="https://tryingtofly.deviantart.com/art/Teddie-s-space-refuge-530950399">Teddie's Space Refuge by tryingtofly</a>.
	</small>
</p>]]></content:encoded>
</item>
<item>
<title>Hostile Private Networks in the Cloud</title>
<link>https://www.alexblackie.com/articles/cloud-private-networks/</link>
<guid>https://www.alexblackie.com/articles/cloud-private-networks/</guid>
<pubDate>Sun, 11 Mar 2018 09:00:00 GMT</pubDate>
<author>alex@alexblackie.com (Alex Blackie)</author>
<description></description>
<content:encoded><![CDATA[ 		<div class="note">
  <div class="note--title">Update 2018-07</div>
  <div class="note--body">
    As of July 2018 DigitalOcean <a href="https://www.digitalocean.com/docs/release-notes/2018/private-networking/">has begun rolling out</a>
    true private networking that is isolated per-account. This is a great step
    forward, and I sincerely hope other VPS providers follow suit. Until then,
    this post is still relevant for basically every other provider.
  </div>
</div>

<p>
  A while ago I deployed a blog using DigitalOcean's infrastructure. I used
  separate droplets for the database and web tier, and used the "private IP"
  feature to only allow MySQL traffic over that interface. I noticed the
  private network seemed larger than necessary for just hosting my own stuff,
  so I did a little digging.
</p>

<p>
  <strong>DigitalOcean's "private networking" is quite literally the opposite
  of private.</strong> Your droplet's private IP will be in a shared subnet
  with every other customer's if they end up on the same network as you. Do not
  think for a minute you can "trust the LAN" or anything like that; I'm sure if
  I scanned deeper I could find dozens of unauthenticated Redis and database
  servers people inadvertently have opened to this not-so-private network.
</p>

<p>
  <strong>Unfortunately, this isn't just DigitalOcean's problem.</strong> I
  ran a couple tests from one of my instances on Linode and was shocked to find
  their Private IP feature both assigns the address to the same interface as
  the public IP, and shares the subnet with many other customers.  These are
  just the two providers I have used; I would hazard to guess this is a common
  practice among the smaller non-Openstack cloud providers.
</p>

<hr>

<p>
  To demonstrate this issue, I spun up a droplet in DigitalOcean's TOR-1
  region. The private interface has the entire <code>10.137.0.0/16</code>
  network routed, and I decided to try doing a basic scan to enumerate the
  entire subnet.
</p>

<div class="code">
  <div class="code-title">nmap -sn 10.137.0.0/16</div>
<pre><code>Starting Nmap 6.40 ( http://nmap.org ) at 2018-03-03 17:06 UTC

Nmap scan report for 10.137.0.4
Host is up (0.0037s latency).
MAC Address: F6:BF:AB:FF:ED:AA (Unknown)
Nmap scan report for 10.137.0.5
Host is up (0.0037s latency).
MAC Address: AE:FC:E8:AD:78:7F (Unknown)
[...]
Nmap scan report for 10.137.255.252
Host is up (0.0082s latency).
MAC Address: 1A:73:75:CA:44:1A (Unknown)
<strong>Nmap done: 65536 IP addresses (5588 hosts up) scanned in 229.23 seconds</strong>
</code></pre></div>

<p>
  I have exactly one other droplet in that region, and yet my brand new droplet
  can access <em>over five thousand</em> other customers' servers on a
  supposedly "private" network. This is not only misleading, but the messaging
  makes it sound like you can trust it; I don't even want to try and knock on
  ports to see how many open-to-the-world services are running.
</p>

<p>
  I won't pretend to be all-knowing and condescending: nothing about running
  infrastructure is easy. However, having separate networks per customer would
  make dramatic improvements to the service's scalability and security, and at
  the very least having one VXLAN per customer seems like a painfully obvious
  good practice.
</p>

<p>
  To verify this, I spun up a droplet in two completely separate DigitalOcean
  accounts (I added my personal CC to my work account to test this).
</p>

<p>
  Right off-the-bat I was able to confirm that both droplets had private IP
  addresses in the same subnet range. Then, on one droplet I started <code>nc
  -l 64321</code>, and on the second I tried to <code>nmap</code> the private
  IP of my second droplet. I was successful.
</p>

<div class="code">
  <div class="code-title">nmap -sS -p64321 10.132.146.7</div>
<pre><code>Starting Nmap 6.40 ( http://nmap.org ) at 2018-03-03 21:57 UTC
Nmap scan report for 10.132.146.7
Host is up (0.0056s latency).
PORT      STATE SERVICE
64321/tcp open  unknown
MAC Address: 6A:1E:6E:F2:92:59 (Unknown)

Nmap done: 1 IP address (1 host up) scanned in 0.19 seconds
</pre></code>
</div>

<p>
  And here's an obligatory screenshot. The left-side droplet is running under
  my personal account, the right-side droplet is under a completely different
  account (but the droplets are in the same region).
</p>

<figure>
  <a href="https://cdn.blackie.zone/alexblackie/private-network-clouds/cross-account-netcat.png">
    <img src="https://cdn.blackie.zone/alexblackie/private-network-clouds/cross-account-netcat.png" alt="Terminal showing netcat communication between droplets in separate accounts">
  </a>
</figure>

<p>
  Luckily my further attempts to <code>tcpdump</code> the traffic between the
  two hosts were unsuccessful. But one could still do a fair amount of damage
  if someone entrusted a service like Redis or NFS to allow connections from
  the entire network of the private interface.
</p>

<hr>

<p>
  I take great issue with these features being labelled "Private Networking" as
  there is nothing private about them whatsoever. There's nothing stopping me
  from spinning up a new server in the same region as you and suddenly I am on
  the same "private" network as you.
</p>

<p>
  By calling this feature "private", it implies privacy. This may mislead
  people into setting up unsecured services because they assume the network
  they are exposing them on is only for their consumption. As long as "hostile
  LAN" practices are followed, having a public private network should be
  <em>fine</em>, but I would strongly urge these cloud providers to implement
  at the very least customer isolation, if not full OpenStack-style SDN.
</p>

<p>
  I hope this post serves as a warning to anyone running services on the
  "underdog" cloud providers: your private network is absolutely not private,
  and you may have an unforseen attack vector left wide open.
</p>]]></content:encoded>
</item>
<item>
<title>Taking Control of Bell Fibe</title>
<link>https://www.alexblackie.com/articles/fibe/</link>
<guid>https://www.alexblackie.com/articles/fibe/</guid>
<pubDate>Sat, 23 Dec 2017 09:00:00 GMT</pubDate>
<author>alex@alexblackie.com (Alex Blackie)</author>
<description></description>
<content:encoded><![CDATA[ 		<p>
  Consumer ISPs are some of my least favourite companies to deal with. Their
  arrogance and lack of competent support makes for an infinitely-frustrating
  experience for someone like me who has managed a network or two. The pain of
  contacting support with very specific, technical explanations of the exact
  problem you're experiencing only to be told to "restart your PC" makes one's
  hair begin to fade.
</p>

<p>
  I recently moved from the west coast of Canada to the east, and one of the
  benefits gained there was fibre-to-the-apartment availability (something the
  west is just <em>barely</em> beginning to know). Coming from a history of
  cable modems, I was excited to see what was different. I had always dreamed
  of replacing the cable modem with some sort of PCI card in my 1U rack server
  I use for a router, but the complexities involved were always not worth it.
  Fibre, however, is a common standard in datacenter-grade equipment, so I was
  looking forward to maybe finally being able to bypass the horrid consumer
  trashbox Bell was likely to give me.
</p>

<p>
  First however, a little context. Most Fibre connections, from what I can
  tell, are treated by the ISP as DSLs (most well known perhaps through the
  proliferation of the old telephone-based ADSL connections). These connections
  use PPPoE to "log in" to the ISP network and even have a dry-loop phone
  number to tie everything together. While a little crusty and convoluted, it
  works, and I suppose means Bell can reuse existing tooling and expertise
  between ADSL and Fibre, so whatever -- it works, at least.
</p>

<p>
  The first thing I did when the Bell technician left was to undo all the
  things he did and figure out what I was dealing with. I was pleased to see
  the drop from the wall was just standard SFP -- turns out the switch I have
  has two SFP ports! I felt a bit smug, I must confess, as when I fielded the
  question of "can I replace the HomeHub entirely?" the tech was quick to
  confidently tell me "it's impossible."
</p>

<p>
  The gateway ("HomeHub 3000") I was provided had no "bridge mode", but it did
  have an "Advanced DMZ" which let me DHCP a public IP to one device.
  Unfortunately, it would provide the DSLAM private IP address as the gateway,
  thus making the connection unusable until I manually set the gateway to be
  the first address in the subnet... I understood <em>why</em> this happened
  (the gateway on the HomeHub itself would be the DSLAM address and that would
  be fine, since it has the routes over the PPP tunnel), and it let me use my
  own router sort of, but it still bugged me that this HomeHub thing was
  running, probably vulnerable, and it all felt like a giant hack.
</p>

<p>
  So after a bit of research, I found luckily a few other Bell Fibe customers
  had done the gruntwork and I found some obscure forum posts from years ago
  where people had got their enterprise routers working directly using the SFP
  drop from the ONT.
</p>

<p>
  Using information gleaned from a few different sources, I was finally able to
  get my OPNSense machine to get a solid PPPoE connection and a routable
  gateway. Success! The basic setup looks something like this:
</p>

<ul>
  <li>SFP from the ONT in the closet plugs into an SFP slot on my managed switch</li>
  <li>Ethernet Ports 1 &amp; 2 on the switch are in a LACP LAGG</li>
  <li>The LAGG is set to pass untagged traffic over VLAN 35</li>
  <li>The SFP slot is set to tag packets explicitly with VLAN 35</li>
  <li>
    The OPNSense WAN interface is set to IPv4 PPPoE
    <ul>
      <li>I configured the LACP LAGG within OPNSense</li>
      <li>I created a disabled OPT interface and associated it to the LAGG interface (for whatever reason, OPNSense/pfSense does not show LAGG devices for PPPoE, but it does show all interfaces)</li>
      <li>The username is set to the one configured in the HomeHub</li>
      <li>The password is one I set myself from within my Bell account settings on their website</li>
      <li>The interface for the PPPoE device is set to the OPT interface I created</li>
    </ul>
  </li>
  <li>I had to do a "Save &rarr; Apply Changes" cycle on the WAN interface after configuring everything to force it to reconnect, and going back to the dashboard I had a green gateway and public IPv4 address.</li>
</ul>

<p>
  I was somewhat hoping that if I swapped in my own hardware I would be able to
  find a way to negotiate for an IPv6 allocation (as the sales rep I spoke to
  before ordering assured me IPv6 was supported), but alas the IPV6CP parameter
  negotiation fails when establishing the tunnel and nothing I've tried has had
  results. At the time of writing, holiday season is approaching its climax, so
  my option is to wait a week or so and complain to support and see what they
  say (maybe it has to be enabled for my account or some garbage like that).
</p>

<p>
  My reasoning for using a LAGG for the WAN connection to the router was
  because I had a gigabit connection, and unfortunately only the budget for
  GigE equipment. By using a LAGG, I avoid transmission overhead over a single
  gigabit wire (which usually means capping around 900mbps), and allows me to
  take advantage of a little bit of burst speed Bell provides (going near
  1.2Gbps).
</p>

<p>
  However, it's all-for-naught, if I am to be honest. I have been unable, using
  my own equipment, to push a full gigabit over the WAN. <em>Something</em> in
  my setup can't handle more than ~750mbps, and as of yet I've been unable to
  diagnose which component it is. My working theory is the SFP ports on the
  switch are only wired at gigabit (whereas my other switches have had 10GbE
  SFP+) and within there lies some sort of overhead or limits.
</p>

<p>
  For now I'm willing to live with "only" 750mbps WAN throughput, but in the
  future I may look into SFP+/10GbE switches as an upgrade path. I was getting
  greater-than-gigabit speedtest results using the HomeHub, so I know the fibre
  connection itself it capable of more than I am able to push through it.
</p>

<hr>

<p>
  Overall, I'm incredibly pleased with how this all turned out. Finally, my
  dream of having an entirely self-controlled home network has come to reality.
  There are no mysterious black boxes to trust, no ISP backdoors into my
  network, and no unpatchable, publicly-exposed embedded devices.
</p>

<p>
  With a couple equipment upgrades, and possibly a call to support to try and
  get IPv6 sorted, this could be a near-perfect setup.
</p>]]></content:encoded>
</item>
<item>
<title>Thinkpad X250: It's actually great.</title>
<link>https://www.alexblackie.com/articles/x250/</link>
<guid>https://www.alexblackie.com/articles/x250/</guid>
<pubDate>Sun, 06 Mar 2016 09:00:00 GMT</pubDate>
<author>alex@alexblackie.com (Alex Blackie)</author>
<description></description>
<content:encoded><![CDATA[ 		<p>
	I have owned a lot of laptops over the past several years. My first laptop was a Dell Inspiron Mini 10, which I loved and just recently restored. Since that laptop I have had a few Macbooks of various sizes and generations, two Thinkpads, and a System76 Galago UltraPro. I knew I definitely preferred the laptop workflow, as opposed to a desktop, but I just never found one that I truly loved -- the laptop that was "just right."
</p>

<p>
	I bought in a clearance sale from a local retailer a base-model Thinkpad T530, quickly thereafter upgrading to an SSD and 12GB RAM with some parts I cannibalized from other machines. My primary laptop at the time was a 13" Retina Macbook Pro, and I wanted a second PC laptop to have as a "play computer" for Linux. The 13" rMBP was what I considered the perfect size for a laptop; the 13" screen was just big enough to be productive, and the slim profile meant it was easy to slip in a sleeve and carry around downtown without issue.
</p>
<p>
	However, I found myself gravitating back to the clearly inferior T530, despite Linux on the Desktop being generally unpleasant and the hardware being incredibly plasticky and heavy. OS X had just become so unusable for me that I couldn't be productive. I fully returned to Linux on the Desktop, and eventually sold my Macbook.
</p>

<p>
	But the quest for a perfect laptop continued. I enjoyed everything about using a Thinkpad again -- the upgradability, the hardware features like ethernet and a smartcard slot, and of course the nipp--err--"TrackPoint&reg;". My only complaint now was that it was <strong>huge</strong>. Nearing six pounds, and too large to carry comfortably in a sleeve, it was just simply too big.
</p>

<p>
	After a misguided purchase of a System76 Galago UltraPro (one of the worst pieces of hardware I have ever owned; a story for another day), I saw a couple friends jump on the X250 when it launched. After a couple weeks of indecision, I caved and ordered one for myself. And thus begins the review.
</p>

<hr />

<p>
	I have used this machine as my primary computer for approximately five months. What has really surprised me is how few complaints I have about it. Usually after a couple weeks the honeymoon effect disipates and I begin noticing things that bug me. With every Macbook I owned that would inveitably be OS X's shortcomings as an operating system, and with every PC laptop eventually it would be because of the dreadful hardware.
</p>

<p>
	But not this time.
</p>

<h3>Screen</h3>

<p>
	Linux on the Desktop can be annoying when it comes to high-DPI resolutions. While 1920x1080 sounds like a great idea, it just is too small, for me at least, to run at 100% at 12.5". I went for the 1366x768 option, and I am glad I did. I find the DPI of the screen to be perfect for this physical size. The backlight is nice and even, and is plenty bright with some users reporting upwards of 392 nits.
</p>

<p>
	I don't do a lot of colour-sensitive work or design, but I find the colours to be about as good as my calibrated <a href="http://www.ncix.com/detail/asus-pa248q-24-1in-widescreen-led-82-73621-1049.htm">Asus ProArt PA248Q</a>, which is the best monitor I have to compare against. <a href="https://github.com/nanotech/jellybeans.vim">Jellybeans</a> looks great, at any rate.
</p>

<h3>Keyboard</h3>

<p>
	Although this isn't the glorious old-style IBM Thinkpad Keyboard, it is still an incredibly comfortable keyboard to type on. Since I type for the majority of every single day, a good keyboard is really one of the most important parts of a laptop. The X250's does not disappoint in the slightest. The key travel is perfect and the key spacing is spot-on.
</p>

<p>
	The Insert key is shared with End via the function layer, as opposed to being a dedicated key. While this is a minor inconvenience for us who use Shift + Insert to paste, I got used to it almost immediately and now it feels perfectly natural.
</p>

<h3>Pointing</h3>

<p>
	In the previous *40 series of Thinkpads, Lenovo did away with the dedicated TrackPoint click buttons, and made the trackpad itself become one giant mushy, wobbly button. I tried it in stores and on a friend's laptop and it was just awful. I was glad to see that for the *50-series they brought back a crisp trackpad and the deciated TrackPoint keys, returning the Thinkpads to the top of the pointing master race.
</p>

<p>
	Since my first Thinkpad I have been a fan of the TrackPoint navigation device, and its glory lives on still.
</p>

<p>
	The decently-sized trackpad is also very pleasant to use. I would say it's nearly comparable to a Macbook's. I haven't noticed any jumpiness or errors while using it, and it supports multi-finger gestures and smooth scrolling. While it is still clearly inferior to the TrackPoint, it is decent enough that I would feel completely comfortable using it in the absence of a TrackPoint.
</p>

<h3>Battery</h3>

<div class="figure">
	<img src="//cdn.blackie.zone/alexblackie/x250/twenty.jpg" alt="nearly twenty hours of battery" />
</div>

<p>
	One of my biggest reservations with switching back to a PC laptop was battery life. With my Macbook, I was able to go for an entire workday without worrying about where my charger was. My T530 got a reasonable four or five hours, but that is still far behind the 12 to 14 hours I could pull out of my Macbook Pro. This is one of the key things that drew me to the X250 to begin with: Lenovo advertises a <strong>twenty-hour</strong> battery life.
</p>

<p>
	For an extra $5, you can bump up to the six-cell, 72 watt-hour battery (the "hump" battery). This, when paired with the internal 3-cell, 23 watt-hour battery, has given me between 15 - 20 hours depending on various factors and workloads. I would say the twenty hour estimate that Lenovo gives is definitely on the high-side, but also completely attainable with some frugal radio usage and brightness settings.
</p>

<p>
	I fear that whatever laptop I move to after this one will be unable to live up to the precendent set by the X250.
</p>

<h3>The Performance</h3>

<p>
	I have the mid-range configuration, with a 128GB SSD, i5-5300U, and 8GB RAM. I find the i5 to be a bit slow, but it has been a long time since I last used an ultrabook-class CPU (the one good thing about the System76 Galago was the i7-4750HQ). I use the word "slow" begrudgingly, as this machine acts a desktop-replacement for me and handles nearly everything without issue. It's only when I really start hammering the system with a few VMs, Chrome, high-resolution video, etc. that it starts to lug a bit. Keeping in mind it's a laptop, my expectations for performance have been met.
</p>

<p>
	The graphics horsepower is pretty useless for anything more than basic desktop composition. It's definitely powerful enough to drive a couple external displays (I've even successfully used an external 60Hz 4K display), run a fancy desktop environment, and stream high-resolution video. However even light gaming is barely passable. This is a fantastic productivity workstation, but definitely not a video or gaming machine.
</p>

<h3>Connectivity</h3>

<p>
	One of the other classic Thinkpad advantages is the plethora of connections available. No single USB-C for this guy.
</p>

<p>
	Gigabit ethernet on-board was a huge feature, coming from a retina Macbook Pro. I find using WiFi to usually be a disappointing or frustrating experience, so being able to get decent LAN speeds and reliable latency is nice. Ethernet isn't going anywhere any time soon.
</p>

<p>
	One thing that surprised me was the VGA port. For starters, it <em>is not</em> actually VGA, but rather DisplayPort disguised as VGA. This means that, with a decent VGA cable, the picture is perfectly clear and totally useable. The VGA port is what allows me to use this machine as a desktop-replacement, as I can now have two external monitors connected when I have the machine on my desk at home.
</p>

<p>
	The rest is fairly standard; I didn't go for a smartcard reader in mine, but now I kind of wish I did. There is also an option to include a LTE modem, which I also did not go for but wish I did.
</p>

<h3>Weight</h3>

<p>
	This machine is not considerably lighter than other 12.5" laptops -- Lenovo states 2.88lbs for the base model, so add an extended battery to that and you're looking at around 3.3lbs. To put that in perspective, the Retina Macbook Pro is ~3.4lbs (but is a slightly larger machine, and is made of metal and glass).
</p>

<p>
	3.3lbs is not too bad. I carry this around with me almost everywhere I go and never do I feel fatigued by it or notice its weight. After packing around a close-to-six pound machine for a while, I'll gladly take 3.3.
</p>

<hr />

<p>
	It has taken me several years and a lot of buying and selling, but I think I have finally found the laptop that does everything I need it to while staying wonderfully small and portable. The X250, for me, is the perfect balance. The battery life lifts a huge burden -- I can slip the machine into a sleeve and be out and about all day without even thinking about how much battery I have left. The performance is adequate to the point where I believe that this machine will stay fast enough for at least another couple years to be useful. The servicability means I can swap out a larger SSD or one of the rumoured 16GB DDR3 SODIMMs that have been floating around in the future&hellip;
</p>

<p>
	The X250 is one of the best laptops I have ever used, and I highly recommend it to writers, programmers, sysadmins, or anyone who has a business or text-driven workflow. I'll be clutching onto mine for hopefully years to come -- I haven't been able to say that about any other laptop I have owned.
</p>]]></content:encoded>
</item>
<item>
<title>AWS VPCs: Getting Started</title>
<link>https://www.alexblackie.com/articles/vpc/</link>
<guid>https://www.alexblackie.com/articles/vpc/</guid>
<pubDate>Sun, 08 Nov 2015 09:00:00 GMT</pubDate>
<author>alex@alexblackie.com (Alex Blackie)</author>
<description></description>
<content:encoded><![CDATA[ 		<p>Amazon Web Services is an incredibly complex environment. Most of its features
no one will ever use, and accomplishing what you assume to be trivial tasks can
take a considerable amount of time and energy before anything works (or you give
up). The AWS veterans I know complain about VPCs constantly, and rightfully so.
Setting up a VPC properly involves a lot of steps and it&#39;s nearly impossible to
do anything without the knowledge of all the components and how they interact.
I am of the opinion VPCs are woefully over-designed and doubt that much of the
functionality is of much use to anyone that is using the Cloud. If you need
<em>that much</em> control over your network, you likely have the means, and would be
better off, to throw a bunch of Cisco gear in a rack somewhere.</p>

<hr />

<p>This guide aims to illustrate how to get a working VPC up and ready for
deployment as fast as possible. None of it is particularly difficult, it is more
just a lot of small things that need to be clicked, checked, or attached.</p>

<p>For demonstration purposes, we will be assuming this VPC will hold a web
application deployed via Opsworks. The majority of the steps here will work for
nearly any deployment, however.</p>

<p><strong>Note</strong>: we will not be covering NAT here. This guide assumes all your servers
will get public IPs and you will allow public SSH to them. NAT introduces yet
another level of complexity and is out-of-scope for this document.</p>

<h3>0. Exploring VPCs</h3>

<p>Before we go off clicking buttons, we should first understand everything that
makes up a VPC, why we need each bit, and how each thing relates to the rest of
the system.</p>

<ul>
  <li><strong>VPC</strong> - The obvious first concept is that of the VPC. A VPC, or &quot;Virtual
  Private Cloud&quot;, is Amazon&#39;s word for what is essentially a container around a
  network. A VPC is the highest-level and is an umbrella around all other
  resources.</li>
  <li><strong>Subnet</strong> - A subnet is essentially a DHCP server for a chunk of your VPCs
  IP range. Instances must go in a subnet, and are assigned internal IP
  addresses from it.</li>
  <li><strong>Security Group</strong> - These can be thought of kind of like firewalls. Instances
  in one security group cannot talk to instances in another security group
  unless explicitly allowed. Security groups also limit external traffic very
  similarly to a traditional firewall in that you have to allow certain ports
  to receive traffic. If you&#39;ve ever been unable to ping or ssh into your VPC&#39;ed
  servers, it was likely a problem with the security group.</li>
  <li><strong>Route Table</strong> - Each VPC gets a route table that defines how to route
  traffic. Basically exactly the same as regular routing on real networks.</li>
  <li><strong>Internet Gateway</strong> - The magic connection to the internet that for some
  reason has to be added manually. Adding one of these to the VPC allows
  connections to the outside world.</li>
</ul>

<p>That should hopefully be enough so that we can get clicking.</p>

<h3>1. Setting up the VPC</h3>

<p>First off we need to create the VPC itself. Head to the VPC section of the AWS
Console and click &quot;Create VPC&quot;.</p>

<p><img alt="vpc_create" width="702" height="322" src="//cdn.blackie.zone/alexblackie/vpc/vpc_create.png" /></p>

<p>I&#39;ve chosen the name <code>my-vpc</code> and the CIDR block of <code>10.20.30.0/24</code>. These both
are fairly arbitrary and can be nearly anything you want. Depending on how many
machines you plan to launch into your VPC, your CIDR block should reflect that.
If you&#39;re not planning on having 200+ machines per subnet, then assigning a
<code>/24</code> block for the whole VPC is totally reasonable.</p>

<p>A keen eye will find that when creating a VPC, you are also given a DHCP Options
Set, Route Table, and Network ACL by default.</p>

<p><img alt="vpc free stuff" width="548" height="86" src="//cdn.blackie.zone/alexblackie/vpc/vpc_extras.png" /></p>

<p>Other than the Route Table, you can ignore these, for the most part.</p>

<h3>2. Getting to the Internet</h3>

<p>A small detail most people miss, and one that is non-obvious later on, is a VPC
has no internet access by default. If you launch an Opsworks Instance into a VPC
without an internet connection, it will hang on boot for a very long time until
it eventually fails.</p>

<p>So let&#39;s add an Internet Gateway.</p>

<p>Click on &quot;Internet Gateways&quot; in the sidebar, and create one. I usually name mine
the same as the VPC it&#39;s going to be attached to.</p>

<p><img alt="create igw" width="703" height="228" src="//cdn.blackie.zone/alexblackie/vpc/igw_create.png" /></p>

<p>Once created, select it and click &quot;Attach to VPC&quot;. Then select your VPC from the
dropdown.</p>

<p><img alt="attach igw" width="659" height="239" src="//cdn.blackie.zone/alexblackie/vpc/igw_attach.png" /></p>

<p>But wait, there&#39;s more! We still haven&#39;t told anything to <em>use</em> the IGW, we&#39;ve
just told it to associate itself with a VPC.</p>

<p>Head to the &quot;Route Tables&quot; section via the sidebar and select the route table
for your new VPC. A pane will open at the bottom of the screen; navigate to the
&quot;Routes&quot; tab and add a route for <code>0.0.0.0/0</code> that points to the new IGW (the
target field should auto-complete when you focus it).</p>

<p><img alt="add igw route" width="671" height="235" src="//cdn.blackie.zone/alexblackie/vpc/route_to_igw.png" /></p>

<p>Congratulations, you just saved yourself hours of painful debugging later.</p>

<h3>3. Subnets</h3>

<p>Probably the most important part of the VPC is the subnet. A subnet is
essentially the same as its physical-network counterpart. Each subnet gets a
slice of the VPC&#39;s CIDR block, and resources launched in the subnet will be
assigned an IP in that range via DHCP automatically.</p>

<p>Since we allocated one Class-C block for our VPC, we&#39;re going to use <code>/26</code>
blocks for our subnets, which is &frac14; of a Class-C. Each quarter will give us
64 addresses (59 usable because AWS reserves a few), which should be plenty,
even for large-scale deployments.</p>

<p>Following with the web application deployment scenario, we&#39;ll need a database
subnet and a subnet for our web servers. RDS requires us to have at least two
subnets for database instances, spread across at least two availability zones</p>

<p>Our network will end up looking something like this:</p>

<ul>
  <li><code>web01</code>: <code>10.20.30.128/26</code>, <code>us-east-1b</code></li>
  <li><code>database01</code>: <code>10.20.30.0/26</code>, <code>us-east-1b</code></li>
  <li><code>database02</code>: <code>10.20.30.64/26</code>, <code>us-east-1c</code></li>
</ul>

<p><img alt="subnets created omg" width="1059" height="139" src="//cdn.blackie.zone/alexblackie/vpc/subnets_created.png" /></p>

<p>For the <code>web01</code> subnet, select it and under the actions dropdown select &quot;Modify
Public IP&quot; and check the checkbox. We will want public IP addresses on all of
our web servers, since we&#39;re not setting up any NAT stuff.</p>

<h3>4. Security Groups</h3>

<p>Finally, the last VPC component we need to configure is Security Groups.
Security Groups act as a sort of firewall, filtering which ports and which types
of traffic are allowed to pass through from the Scary Internet&trade; to the
VPC.</p>

<p>When creating a VPC, we get a default security group aptly named <code>default</code>. We
are now faced with a decision: how much do we <em>really</em> care? Since the goal of
this document is to get a VPC set up as fast as possible and with as little
friction as possible, setting up a set of custom security groups is not going to
get us to our goal any sooner, and provides only minimal benefit to us at the
moment. Launching everything in the <code>default</code> security group is <em>totally fine</em>
if you don&#39;t care about limiting inter-machine communication. For critical
production setups, I would recommend looking into security groups as an extra
level of security between your various layers of services, but for now <code>default</code>
will serve us just fine.</p>

<p>With that out of the way&hellip;</p>

<p>Select the <code>default</code> security group for your VPC, and navigate to the &quot;Inbound
Rules&quot; tab. You can see the one default rule allows all traffic between machines
within the security group. We need to poke a couple holes for the services we
want to be able to access publicly.</p>

<p>Following with the Opsworks theme, I&#39;m going to open up ports for HTTP and SSH
from any IP address.</p>

<p><img alt="opening http and ssh" width="750" height="280" src="//cdn.blackie.zone/alexblackie/vpc/sg_open_ports.png" /></p>

<p>The &quot;Source&quot; fields will attempt to auto-complete names of security groups in
your VPC. Since we&#39;re entering an IP range, it will complain about &quot;No results&quot;
-- you can ignore these complaints.</p>

<h3>Coffee break</h3>

<p>You&#39;ve now set up a VPC worthy of being used. Now you can start launching RDS
instances, setting up Opsworks apps, perhaps even exploring security groups more
and learning how to lock down your internal traffic. Perhaps you&#39;re feeling
inspired and want to take it a step further with NAT instances and private
LANs...</p>]]></content:encoded>
</item>
<item>
<title>Quick and Dirty Containers with systemd</title>
<link>https://www.alexblackie.com/articles/nspawn/</link>
<guid>https://www.alexblackie.com/articles/nspawn/</guid>
<pubDate>Wed, 22 Jul 2015 09:00:00 GMT</pubDate>
<author>alex@alexblackie.com (Alex Blackie)</author>
<description></description>
<content:encoded><![CDATA[ 		<p>
  Docker and other containerization technologies are making the rounds in the
  Linux community, but a lowly hero lurks beneath now every major Linux
  distro; enter <code>systemd-nspawn</code> containers.
</p>

<p>
  systemd has arrived with mixed reviews -- you either love it or you hate it
  -- but one thing stands for certain: it gets the job done. Regardless of
  your emotions projected towards it, systemd is likely here to stay for a
  while, so you might as well exploit as many features as you can out of it.
</p>

<p>
  <code>systemd-nspawn</code> containers are akin to FreeBSD Jails more than
  Docker containers. They&#39;re basically just a fancy <code>chroot</code>
  with some handy built-in integrations with systemd. You can start, stop,
  enable, and disable the containers as if they were regular services.
</p>

<p>
  Keep in mind, however, that by its own admission,
  <code>systemd-nspawn</code> is an experimental feature that hasn&#39;t been
  thoroughly tested or audited. There are no guarantees of security or
  stability; it&#39;s probably best to keep them out of production for the
  time being. That said, here at BlackieOps we&#39;ve been using
  <code>systemd-nspawn</code> containers for Jenkins, Stash, and Jira for a
  while now without any issues.
</p>

<h3>Creating the container</h3>

<p>
  We will be working on a fresh install of CentOS 7, but this process is
  possible on any system using systemd. The only difference will be the first
  package installation step. Obviously, on Debian you will not be using
  <code>yum</code>, and on Fedora your repo names and release versions will
  be different (and you&#39;ll be using <code>dnf</code>)&hellip;
</p>

<hr />

<p>
  First step is to &quot;install&quot; a new root filesystem into a
  directory.
</p>

<pre><code># yum -y --releasever=7 --nogpg --installroot=/var/lib/machines/cool-container   --disablerepo='*' --enablerepo=base install systemd passwd yum centos-release
</code></pre>

<p>
  This will create a directory,
  <code>/var/lib/machines/cool-container</code>, and populate it with a new root
  filesystem and a couple core packages.
</p>

<p>
  Second, we need to enter into the container to set up some basic things
  like the <code>root</code> password. We can use the
  <code>systemd-nspawn</code> command directly for this:
</p>

<pre><code># systemd-nspawn -D /var/lib/machines/cool-container</code></pre>

<p>
  This drops you to a shell in the container without actually
  &quot;booting&quot; anything inside of it (think of it as like
  <code>chroot</code>-ing from a recovery mode. From here, we can set the
  root password so we can log in later.
</p>

<p>
  When you&#39;re done, just <code>^D</code> out as usual and you&#39;ll be
  dropped back to your host machine.
</p>

<h3>Aside: kernel auditing and containers</h3>

<p>
  If you ignore this section and continue trying to boot the container, you
  will likely get a warning before the container starts about the kernel
  auditing subsystem. There are supposedly odd bugs that can surface if
  auditing is enabled, so we&#39;re just going to disable it. If this worries
  you, feel free to inspect the issue further, but since this is not a
  production system it&#39;s probably fine.
</p>

<p>
  We just need to add a flag to the kernel parameters in our bootloader. This
  will vary between distros, but for CentOS it&#39;s as easy as editing the
  <code>/etc/sysconfig/grub</code> file and changing the
  <code>GRUB_CMDLINE_LINUX</code> variable by appending <code>audit=0</code>
  to the list of parameters.
</p>

<p>
  After editing the parameters, we&#39;ll need to regenerate our GRUB
  configuration:
</p>

<pre><code># grub2-mkconfig -o /etc/grub2.cfg</code></pre>

<h3>Configuring the base system</h3>

<p>
  We now have a skeleton of a container installed, but we still need to
  actually configure what&#39;s inside of it, and get it prepped to start
  automatically, or at least as a service from systemd.
</p>

<p>
  Since we now have access to the <code>root</code> account, we can fully
  &quot;boot&quot; the container:
</p>

<pre><code># systemd-nspawn -bD /var/lib/machines/cool-container
</code></pre>

<p>
  The <code>-b</code> flag is short for <code>--boot</code> and basically
  means <code>systemd-nspawn</code> will search for an init binary and
  execute it. You&#39;ll see the standard boot log fly by, and then be
  dropped at a standard PTY login prompt. Log in with the root credentials
  you set up previously, and now we can start installing things as if we were
  on a brand new machine.
</p>

<p>
  Once you have your container set up and everything is running and
  configured, you can exit by &quot;shutting down&quot; the container as if
  it was a physical machine: <code>poweroff</code> or <code>halt</code> (or
  whatever you usually use).
</p>

<h3>Managing the container</h3>

<p>
  While the <code>/var/lib/machines</code> prefix at the beginning may have
  seemed arbitrary, in fact it was intentional -- containers in this
  directory will be auto-discovered by systemd and we can enable and manage
  them automatically.
</p>

<p>To have your container start with everything else when your host boots:</p>

<pre><code># systemctl enable systemd-nspawn@cool-container</code></pre>

<p>
  And as you can perhaps guess, we can start and stop our container just as
  any other service:
</p>

<pre><code># systemctl start systemd-nspawn@cool-container
# systemctl stop systemd-nspawn@cool-container</code></pre>

<h3>Accessing the container</h3>

<p>
  Accessing a running container can be a bit tricky; one option is to install
  <code>openssh</code> in the container and have it run on a non-standard
  port (as containers share the host&#39;s network interfaces).
  Alternatively, you can access the machine through <code>machinectl</code>.
</p>

<p>
  Just running <code>machinectl</code> without arguments will list all
  running containers (and other VMs, etc). Interestingly, the older version
  of <code>machinectl</code> on CentOS does not allow us to use the
  <code>login</code> argument (so you may want to install
  <code>openssh</code>)... If you&#39;re on Fedora (or a different more
  up-to-date distro), we can use <code>machinectl login</code> command:
</p>

<pre><code># machinectl login cool-container</code></pre>

<p>... which will drop us at that familiar PTY prompt.</p>

<p>
  Since we don&#39;t necessarily want to halt the container to escape from
  this prompt, there is a panic button to disconnect: hit escape three times
  within a second (i.e., fast).
</p>

<hr />

<p>
  In conclusion, <code>systemd-nspawn</code> is an interesting technology
  that shows promise.  Its ubiquity through the proliferation of systemd
  means containers are quite portable, easy to set up, and well-integrated
  directly into the OS&#39;s init system.
</p>

<p>
  Would I use it in production? Probably not. It&#39;s a very green
  technology and its immaturity is worrisome enough that my sleep cycles
  would be lessened dramatically by its deployment. For production
  &quot;containers&quot;, FreeBSD Jails still provide the best security and
  featureset.
</p>

<p>
  For now, <code>systemd-nspawn</code> is staying on my internal
  infrastructure, running my Atlassian stack, Jenkins, etc.; and it is
  running those internal services quite well. But until its features are more
  solidified and someone has verified it is at least moderately secure, it
  won&#39;t be finding its way to my production stack for a few years
  yet.
</p>]]></content:encoded>
</item>
<item>
<title>Asus X205TA Netbook Review</title>
<link>https://www.alexblackie.com/articles/x205ta/</link>
<guid>https://www.alexblackie.com/articles/x205ta/</guid>
<pubDate>Fri, 14 Nov 2014 09:00:00 GMT</pubDate>
<author>alex@alexblackie.com (Alex Blackie)</author>
<description></description>
<content:encoded><![CDATA[ 		<figure>
  <img src="https://cdn.blackie.zone/alexblackie/x205ta/the-machine.jpg" alt="The Machine." />
</figure>

<p>
  I've always been a fan of laptops. I love having the ability to pick up my
  computer and go work somewhere else for a while without interrupting my
  workflow and without having to sync files between a laptop and desktop.
  Right now my daily-driver is a 15" Thinkpad T530: it's large, thick, and
  heavy, but it's a wonderful desktop-replacement and I love it for
  everything but its size.
</p>

<p>
  Due to the hulking nature of my primary laptop, I wanted something super
  light and (preferrably) cheap to take to coffee shops, meetups, etc. â€“
  places that weren't as accomodating to a 15" beast of a workstation. I
  scoured the internet, falling on many perfect-sounding products (like the
  HP Stream) that simply <em>are not</em>, at the time of writing, available
  to purchase in Canada.  Eventually I found myself on Microsoft's online
  store, and minutes later I had ordered one of the 11" Asus notebooks
  (netbook?). All told, the order came to a whopping $200 (after tax and
  shipping costs).
</p>

<h3 id="table-of-contents">Table of Contents</h3>

<ul>
  <li><a href="#box">The Packaging</a></li>
  <li><a href="#hardware">The Hardware</a></li>
  <li><a href="#performance">The Performance</a></li>
  <li><a href="#software">The Software</a></li>
  <li><a href="#usecase">The Use Case</a></li>
</ul>

<p><a name="box"></a></p>

<h3 id="the-packaging">The Packaging</h3>

<p>Let's start with something trivial and boring: the box.</p>

<figure>
  <img src="https://cdn.blackie.zone/alexblackie/x205ta/box.jpg" alt="The Box" />
</figure>

<p>
  The packaging is all cardboard, and quite eco-friendly (except for the
  styrofoam pouch wrapping the computer itself). +10 environment points for
  Asus.
</p>

<p>
  The box design is covered in "IN SEARCH OF INCREDIBLE", which are
  definitely English words but I still don't think it actually means
  anything.
</p>

<p><a name="hardware"></a></p>

<h3 id="the-hardware">The Hardware</h3>

<p>
  For a $170 computer, I had zero expectations. I have used low-end PC
  laptops before, so I sort of assumed it would be creaky, bendy, and nearly
  impossible to use without feeling gross.
</p>

<p>Simply put, I was wrong.</p>

<h4 id="the-keyboard">The Keyboard</h4>

<p>
  I want to start with what is probably the highlight of this machine: the
  keyboard. The chicklet-style keys have a moderate travel distance, a
  slightly textured feel, and a very satisfying crispness. To boot, there is
  absolutely no bend to the keyboard at all â€“ it is extremely solid.
</p>

<p>
  Only a couple keys got shafted: the escape and tilde keys are both
  half-width, but that's acceptible on such a tiny form-factor, and for
  generally under-used keys. Otherwise, the keyboard feels full-size (or
  damn-well close to it) and I have had no trouble typing this entire review
  using it.
</p>

<h4 id="the-body">The Body</h4>

<p>
  The body on this machine is totally solid. I can't seem to find any points
  of flex, nor any creaky or cheap-feeling plastic. The entire exterior is a
  type of soft-touch plastic that feels wonderful to the touch.
</p>

<p>
  The lid has a bit of wobble if you tap it, but only slightly more than my
  Thinkpad. There is a single pressure-point that produces visible
  distortions on the display if you close the lid with one hand, but nothing
  I would worry about.
</p>

<h4 id="the-size">The Size</h4>

<p>
  This is one of the smallest and thinnest laptops I've used. It's not quite
  Macbook Air-thin, but it's close. For comparison, here it is beside my 2014
  Retina Macbook Pro.
</p>

<figure>
  <img src="https://cdn.blackie.zone/alexblackie/x205ta/height-open.jpg" alt="Height comparison with Macbook while open" />
</figure>

<p>
  If Apple made an 11" Retina Macbook Pro, this would match its physical size
  almost exactly.
</p>

<p>
  However, its similarities with the Retina Macbook Pro end with the
  dimensions.
</p>

<h4 id="the-screen">The Screen</h4>

<p>
  The screen is a 1366x768 TN panel with a glossy finish. Personally, I dislike
  all of these things. That said, the TN panel is one of the better ones, with
  adequate viewing angles and surprisingly good colours (most likely due to the
  Asus Splendid utility that comes pre-installed). The resolution is also
  acceptible because at 11", the screen has a DPI that actually makes that
  resolution perfect. The glossiness I can get over (I've used Apple products for
  long enough to ignore it), but I still would prefer a matte version.
</p>

<p>
  Overall, it's a <em>good</em> screen. It's not great, but for a $170
  computer, it's pretty good.
</p>

<h4 id="the-trackpad">The Trackpad</h4>

<p>
  The trackpad is another strong point of this laptop. I'd even venture to say
  it's one of the best PC trackpads I've used. It is accurate, smooth, large, has
  a satisfying click, and is multi-touch. There are not separate click buttons,
  which could be annoying for some people, but I found myself clicking with my
  other hand, resting my finger on the bottom left, and it worked fine.
</p>

<p>
  The included Asus gesture-enhancing driver does wonders; two-finger smooth
  scrolling works great, edge-swiping to get to charms works great, tap-to-click
  works great&hellip;
</p>

<p>
  Really, I have no complaints. It works and it works well. It's actually better
  than the one on my Thinkpad.
</p>

<h4 id="the-webcam">The Webcam</h4>

<p>
  This thing has a webcam. It's blurry, grainy, slow, and 480p. It is terrible.
</p>

<h4 id="the-audio">The Audio</h4>

<p>
  The on-board audio is also surprisingly not bad. If you usually use
  on-board audio, you probably will be totally happy with it. If you're on
  the go and need to listen to music, it'll accomodate. From a couple
  listening tests with different pairs of headphones, it outputs a fairly
  balanced and clear sound, if lacking a bit in the low-mid-range. It drives
  my Sennheiser HD280 Pros without a problem, even at (dangerously) high
  volumes. I could push it all the way to 100 without any distortion or
  quality loss. Dance music is punchy and crisp; speed metal maintains its
  definition and doesn't get lost in a white-noise mess; acoustic guitar and
  acapella sound very clear and clean&hellip;
</p>

<p>
  Obviously plugging in an external DAC will always be a better choice, but
  if you're too lazy, too cheap, or too far away the on-board audio does a
  satisfactory job.
</p>

<p>
  I would mention what the chip is, but I can't seem to be able to find out.
  Both <a href="https://www.piriform.com/speccy">Speccy</a> and Windows Device
  Manager report it as an "Intel SST Audio Device", but there is a pre-installed
  "Realtek HD Audio Manager" so I'm not quite sure what it is or who makes
  it.
</p>

<p>
  The in-built speakers are surprisingly not terrible. Granted, they are
  still laptop speakers, and compared to anything else are tinny and
  dreadful, but they have a noticable depth to the sound and are very loud.
  Perfect for podcasts, Youtube, or even light music listening if you hate
  your music. The speakers are down-firing and have little slits in the
  bottom-front of the case to speak out of. The down-firing nature likely
  helps dissapate most of the hissy high-end that is usually so present in
  laptop speakers. It also means, however, that they are completely useless
  when on a lap.
</p>

<h4 id="the-lack-of-vents">The Lack of Vents</h4>

<p>
  One thing I immediately noticed was: there are no holes. No vents, no gaps,
  no grills&hellip; The low-power nature of the CPU is such that it does
  absolutely fine with no active cooling. The lack of vents, to Steve Jobs'
  credit, very much adds to the sexiness of the design.
</p>

<p><a name="performance"></a></p>

<h3 id="performance">Performance</h3>

<p>"But Alex&hellip;" you cry, "how does it <em>perform</em>?!"</p>

<p>Firstly, a quick rundown of the guts.</p>

<ul>
  <li>Quad-core Intel Atom Z3735F (Bay-Trail) @ 1.33Ghz</li>
  <li>2GB DDR3 RAM @ 666Mhz</li>
  <li>32GB internal SD-based storage (21GB useable)</li>
  <li>Some Broadcom Wireless N card</li>
  <li>2 USB3 ports</li>
  <li>1 micro-HDMI port</li>
  <li>1 micro-SD card slot</li>
  <li>1 headphone/mic combo jack</li>
</ul>

<figure>
  <img src="https://cdn.blackie.zone/alexblackie/x205ta/cpuid.png" alt="CPU ID" />
</figure>

<p>"But Alex&hellip;" you cry, "those specs are <em>shit</em>!"</p>

<p>
  They are definitely not the most impressive specs, but that was one of the
  reasons I bought this machine in the first place: I wanted to torture a Bay
  Trail chip to see what the performance was like. <a href="http://ark.intel.com/products/80274/Intel-Atom-Processor-Z3735F-2M-Cache-up-to-1_83-GHz?q=z3735f">Intel ARK reports this chip</a>
  to have a SDP of just over 2W, so just being able to <em>boot Windows</em> would be
  good enough for me to call it a successful part.
</p>

<p>Luckily, it does much more than that.</p>

<h4 id="minecraft">Minecraft</h4>

<p>
  Just for fun, I decided to play a video game. I installed Minecraft,
  generated a new world, and played for a few minutes. On average, I went
  between 15 and 22fps. A <em>lot</em> more than I was expecting. I would
  nearly call that playable, if you were in a pinch and <em>super</em> bored
  and just <em>needed</em> to play <em>something</em> no matter what.
</p>

<p>
  You won't be running Crysis on this any time soon, but hey for a light,
  casual game when you have nothing else, this works enough.
</p>

<h4 id="the-cpu">The CPU</h4>

<p>
  The Bay Trail Atom performs pretty well, but it definitely doesn't break
  any speed records.
</p>

<p>
  <a href="http://browser.primatelabs.com/geekbench3/1269677">GeekBench 3
  scores</a> a 702 and 2147 for single- and multi-core, respectively.
  It's quite obvious that this chip is a serious multi-tasker, which likely
  helps when running a full OS that generally has a tonne of background
  services and applications running.
</p>

<h4 id="the-storage">The Storage</h4>

<p>
  Interestingly, it appears this laptop uses SD-based internal storage (yes,
  <strong>SD</strong>)&hellip; Inside, as the primary storage and OS drive, is a
  Hynix (Hitachi) 32GB SD card (is it really still a card if it's permanently
  internal?).
</p>

<p>
  Also interesting is the fact is comes out-of-the-box with Bitlocker enabled. I
  hope this becomes standard for all computers.
</p>

<p>
  The performance is about what I'd expect from an SD card. Sequential read is
  pretty good, matching a high-end spinning disk, but everything else is pretty
  mediocre, even dropping to a little over 10MiB/s for random 4k writes&hellip;
</p>

<figure>
  <img src="https://cdn.blackie.zone/alexblackie/x205ta/disk.png" alt="Disk performance benchmark" />
</figure>

<p>
  32GB is pretty small, especially for a full Windows laptop â€“ honestly I'm
  surprised Windows fits on here at allâ€¦ Never has an 8GB recovery partition
  seemed so large&hellip;
</p>

<h4 id="general-use">General Use</h4>

<p>
  I committed to doing everything for an entire day on this machine â€“ including
  taking it into the office and actually doing real work. I never noticed any
  noticable slowdowns or stuttering during use, even when I had a few Chrome tabs
  open with streaming music and a couple heavy webapps like Slack.
</p>

<p>Throughout the day, I did quite a few things:</p>

<ul>
  <li>Streaming 1080p Youtube videos</li>
  <li>Streaming Google Play Music while working through SSH with two Gmail tabs and
Slack open</li>
  <li>Played Minecraft</li>
  <li>Ordered pizza online</li>
  <li>Wrote this review</li>
</ul>

<p>And not only that, but I did it all on battery.</p>

<h4 id="the-battery">The Battery</h4>

<p>
  The battery life of this laptop is by far one of the most appealing features.  I
  unplugged it when I left for work at 07:30, and didn't plug it back in until I
  got home around 17:00, still with just under 40% battery life remaining. 40%
  battery remaining after a full day of active SSH sessions, heavy browser usage,
  and near-constant screen-on time.
</p>

<p>
  The insane power-sipping tendencies of the Atom make battery life a completely
  non-issue, getting into Macbook territory â€“ in fact, if Asus's marketing is
  correct (and all signs point to "yes"), this laptop has a 12-hour battery life,
  which matches that of the 2014 13" Macbook Air, and is <em>greater</em> than that of
  the 2014 11" Macbook Air. Granted, the performance is not at par, but it is a
  huge step up from the usual three hours your average PC laptop will achieve.
</p>

<p><a name="software"></a></p>

<h3 id="the-software">The Software</h3>

<p>
  Out-of-the-box, we get Windows 8.1 "with Bing" (I'm still not sure what
  differentiates it from regular Windows 8.1). It's everything you love (and
  probably don't love) about Windows, plusâ€¦ hold onâ€¦ it must be here
  somewhere&hellip;
</p>

<h4 id="bloatware">Bloatware</h4>

<p>
  Perhaps the most surprising thing about this laptop is the <em>lack</em> of
  bloatware.  There are three Asus-branded apps, all of which I never noticed
  until I looked in the "Programs and Features" control panel because they
  all do something to make the experience better, like multitouch gestures on
  the trackpad or colour correction for the display&hellip;
</p>

<p>
  Aside from all the bullshit Metro apps that come with even the OEM versions
  of Windows, there was no bloatware, adware, or trials. It was
  astonishingâ€“and refreshingâ€“to find.
</p>

<p>
  Suffice it to say, the "Microsoft Signature Edition" laptops definitely
  have my vote, especially for computers with 32-bit EFIs&hellip;
</p>

<h4 id="bit-efi">32-Bit EFI</h4>

<p>
  Here comes the biggest kicker of them all: the laptop ships with a
  <strong>32-bit version</strong> of Windows. Yes, 32-bit. The Atom itself is
  a 64-bit CPU, but both Windows and the UEFI are 32-bit. From what I've read
  on it, it's a memory/space-saving measure to get the most out of the measly
  2GB RAM, but still something to note.
</p>

<p>
  <strong>But there's a larger problem</strong>: Linux support for 32-bit
  UEFI is, well, nonexistent. I found a couple unofficial Debian ISOs from
  2012, but nothing else. There are many forum threads about it, and it seems
  to be a common problem amoung these low-power Atom CPUs.
</p>

<p>
  I tried for a couple hours to just <em>boot</em> a live USB, but to no
  avail. I simply could not get Linux to boot properly under a 32-bit EFI.
  I'm hoping with the proliferation of more of these small Atom machines and
  tablets that more attention is given to GRUB and related projects such that
  in the future the support will be there (or maybe a firmware patch from
  Asus to enable 64-bit booting, imagine that)&hellip;
</p>

<p>
  It's also worth mentioning that there is no legacy BIOS support, meaning
  even if you wanted to run a 32-bit Linux distro, you <em>still</em>
  couldn't boot it because the 32-bit distros use the legacy BIOS due to the
  32-bit EFI support problems.
</p>

<p>
  This is by far the killer for the machine. I originally bought it with the
  intention of installing Linux and this bizarre limitation has struck that
  down, at least for the time being.
</p>

<p><a name="usecase"></a></p>

<h3 id="conclusion">Conclusion</h3>

<p>
  With power like this at a price like this, and now with everything moving
  to be internet-based services, I can see a resurgence in the "netbook"
  computer market. The performance of the Bay Trail chip has me very
  impressed, and I can't wait to work this computer into my day-to-day
  workflow.
</p>

<p>
  For me, I just wanted a simple, cheap, portable secondary laptop to take
  with me places instead of my beast of a Thinkpad. This computer fits that
  use-case <em>perfectly</em>. It has surpassed my expectations for a $170
  netbook.
</p>

<p>
  I could recommend this laptop to students, or even business-people. If all you
  need is a super-portable machine to do email and some office-type work, this
  thing is perfect. Or if you have a powerful desktop at home and a VPN, you could
  use this as your remote machine and just work over SSH with tmux (as I do).
</p>

<p>
  Or, hell, just as a fun play machine because it's $170.
</p>]]></content:encoded>
</item>

</channel>
</rss>
